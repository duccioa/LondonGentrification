{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatial Analysis of Food Business Data\n",
    "\n",
    "### Exploring gentrification through consumer preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#LOAD REQUIRED PACKAGES\n",
    "\n",
    "#Stats and data structures\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "#Matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib.collections import PatchCollection\n",
    "from descartes import PolygonPatch\n",
    "\n",
    "#Coordinate system transformation\n",
    "import pyproj\n",
    "from pyproj import Proj, transform\n",
    "\n",
    "#Shapefile reading and manipulataion\n",
    "import shapefile\n",
    "import shapely\n",
    "from shapely.geometry import Polygon\n",
    "from shapely.geometry import Point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Data import and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Set working directory\n",
    "import os\n",
    "os.chdir(\"C:/Users/Claire/Google Drive/LondonGentrification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import .csv data \n",
    "\n",
    "#Word tokens \n",
    "tokens_df = pd.read_csv(\"Data/FoodPremises/tokens_spatial.csv\")\n",
    "\n",
    "#Food businesses\n",
    "food_bus_df = pd.read_csv(\"data/FoodPremises/london_premises.csv\")\n",
    "#Remove records with no coordinates\n",
    "food_bus_df = food_bus_df.loc[food_bus_df['Latitude']>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BusinessID                float64\n",
      "BusinessName               object\n",
      "BusinessType               object\n",
      "BusinessTypeID            float64\n",
      "ConfidenceInManagement    float64\n",
      "Hygiene                   float64\n",
      "LocalAuthorityCode        float64\n",
      "LocalAuthorityName         object\n",
      "PostCode                   object\n",
      "RatingValue               float64\n",
      "Structural                float64\n",
      "Token                      object\n",
      "lat                       float64\n",
      "lon                       float64\n",
      "dtype: object\n",
      "Index                       int64\n",
      "BusinessName               object\n",
      "BusinessType               object\n",
      "BusinessTypeID              int64\n",
      "PostCode                   object\n",
      "RatingValue               float64\n",
      "RatingDate                 object\n",
      "LocalAuthorityCode          int64\n",
      "LocalAuthorityName         object\n",
      "Hygiene                   float64\n",
      "Structural                float64\n",
      "ConfidenceInManagement    float64\n",
      "Longitude                 float64\n",
      "Latitude                  float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "#Check column names and data types\n",
    "print tokens_df.dtypes\n",
    "print food_bus_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to add eastings and northings from lat an lon\n",
    "#Add eastings and northings to word tokens\n",
    "\n",
    "def transform_coordinates (data,input_espg, output_espg, input_x, input_y):\n",
    "\n",
    "    #define input and output projection\n",
    "    input_projection = pyproj.Proj(\"+init=\" + input_espg) #wgs84\n",
    "    output_projection = pyproj.Proj(\"+init=\" + output_espg) #osgb36\n",
    "\n",
    "    eastings = []\n",
    "    northings = []\n",
    "\n",
    "    x_list = input_x.tolist()\n",
    "    y_list = input_y.tolist()\n",
    "\n",
    "    for i in range (len(input_x)):\n",
    "        x = x_list[i]\n",
    "        y = y_list[i]\n",
    "        new_x, new_y = pyproj.transform(input_projection, output_projection, x, y)\n",
    "        eastings.append(new_x)\n",
    "        northings.append(new_y)\n",
    "\n",
    "    #Add to tokens dataframe\n",
    "    data['eastings'] = eastings\n",
    "    data['northings'] = northings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Apply funtion to dataframe - convert tokens and businesses to eastings and northings\n",
    "\n",
    "transform_coordinates(tokens_df, \"EPSG:4326\", \"EPSG:27700\", \n",
    "                       tokens_df['lon'], tokens_df['lat'])\n",
    "\n",
    "transform_coordinates(food_bus_df, \"EPSG:4326\", \"EPSG:27700\", \n",
    "                       food_bus_df['Longitude'], food_bus_df['Latitude'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create subsetted dataframes for each token of interest for operations later\n",
    "\n",
    "#List of tokens of interest to perform iteration operations:\n",
    "tokens_lst = ['cafe', 'coffee', 'pizza', 'wine', 'sushi', 'thai', 'chicken', \n",
    "                   'fried', 'fish', 'kebab', 'costcutter', 'waitrose', 'sainsburys', 'tesco', 'grill']\n",
    "\n",
    "#Colour codes to associate with each token\n",
    "tokens_color = ['#9fc54d', '#75c156', '#33a457', '#71b67b', '#61bdf0', '#1e71b8', '#e03c00', \n",
    "                '#e56000', '#f08c00', '#ffea00', '#18563e', '#729f1e', '#ee7a01', '#0053a0', '#1ea86c']\n",
    "\n",
    "df = {}\n",
    "\n",
    "for x in tokens_lst:\n",
    "    df[x] = pd.DataFrame(tokens_df.loc[tokens_df['Token'] == x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Import london wards shapefile and save as matplot lib patches for plotting\n",
    "\n",
    "#Load the shapefile of polygons and convert it to shapely polygon objects\n",
    "polygons_sf = shapefile.Reader(\"Data/ESRI/london_wards.shp\")\n",
    "polygons_south_sf = shapefile.Reader(\"Data/ESRI/south_london_wards.shp\") #Separate shapefile for south london wards\n",
    "polygons_north_sf = shapefile.Reader(\"Data/ESRI/north_london_wards.shp\") #Separate shapefile for north london wards\n",
    "\n",
    "polygon_shapes = polygons_sf.shapes() #Create object with shapes\n",
    "polygon_points = [q.points for q in polygon_shapes ] #Extract point information from shapes\n",
    "polygons = [Polygon(q) for q in polygon_points]\n",
    "\n",
    "#Create matplotlib patches from shapely polygons for mapping figures\n",
    "ward_patches = []\n",
    "for x in range (len(polygons)):\n",
    "    a = PolygonPatch(polygons[x])\n",
    "    ward_patches.append(a)\n",
    "    \n",
    "#Define bounding box of the shapefile (eastings and northings) - plus boundary increase for figures\n",
    "xmin = polygons_sf.bbox[0] - 2000\n",
    "xmax = polygons_sf.bbox[2] + 2000\n",
    "ymin = polygons_sf.bbox[1] - 2000\n",
    "ymax = polygons_sf.bbox[3] + 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create wards dataframe from shapefile\n",
    "\n",
    "#polygons_sf.fields #Access shapefile fields\n",
    "#polygons_sf.records #Access shapefile records\n",
    "\n",
    "records = polygons_sf.records()\n",
    "\n",
    "position = []\n",
    "ward_name = []\n",
    "ward_code = []\n",
    "\n",
    "for x in range (len(records)):\n",
    "    position.append(records[x][0] - 1)\n",
    "    ward_name.append(records[x][1])\n",
    "    ward_code.append(records[x][2])\n",
    "\n",
    "#Compile dataframe\n",
    "ward_variables = pd.DataFrame({'position': position, 'ward_name': ward_name,'ward_code':ward_code})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create field to define wards north and south of the river\n",
    "\n",
    "records_north = polygons_north_sf.records()\n",
    "records_south = polygons_south_sf.records()\n",
    "\n",
    "code_north = []\n",
    "n=[]\n",
    "code_south = []\n",
    "s=[]\n",
    "\n",
    "for x in range (len(records_north)):\n",
    "    code_north.append(records_north[x][2])\n",
    "    n.append('n')\n",
    "    \n",
    "for x in range (len(records_south)):\n",
    "    code_south.append(records_south[x][2])\n",
    "    s.append('s')\n",
    "\n",
    "#Compile dataframe\n",
    "north_wards_df = pd.DataFrame({'code':code_north, 'n_s':n})\n",
    "south_wards_df = pd.DataFrame({'code':code_south, 'n_s':s})\n",
    "north_south_df = north_wards_df.append(south_wards_df)\n",
    "\n",
    "#Merge north south dataframe to ward variables dataframe by ward code\n",
    "ward_variables = pd.merge(ward_variables, north_south_df,left_on='ward_code', right_on='code' , how='inner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Calculate centroids for each ward polygon and add to ward variables dataframe\n",
    "centroids = []\n",
    "\n",
    "for x in range(len(polygons)):\n",
    "    a = polygons[x].centroid\n",
    "    centroids.append(a)\n",
    "    \n",
    "ward_variables['centroid'] = centroids\n",
    "\n",
    "centroid_x = []\n",
    "centroid_y = []\n",
    "\n",
    "\n",
    "for x in ward_variables['centroid']:\n",
    "    centroid_x.append(x.x)\n",
    "    centroid_y.append(x.y)\n",
    "    \n",
    "ward_variables['centroid_x'] = centroid_x \n",
    "ward_variables['centroid_y'] = centroid_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Join income data to ward variables dataframe\n",
    "\n",
    "#Import income .csv\n",
    "ward_income = pd.read_csv(\"Data/modelled-household-income-estimates-wards.csv\")\n",
    "\n",
    "#Change code for city of london to be consistent with shapefile\n",
    "ward_income.set_value(0,['Code'], 'E05001554')\n",
    "\n",
    "#Join to ward variables dataframe\n",
    "ward_variables = ward_variables.merge(ward_income[['Code','Median 2012_13']], left_on='ward_code', right_on = 'Code')\n",
    "\n",
    "#Rename Column\n",
    "ward_variables=ward_variables.rename(columns = {'Median 2012_13':'med_income_2012_13'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Categorise as high or low income (below or above median)\n",
    "\n",
    "#Caluculate median\n",
    "med_income = np.median(ward_variables['med_income_2012_13'])\n",
    "\n",
    "#Define function for categories\n",
    "def income_category (row):\n",
    "    if row['med_income_2012_13'] >= med_income:\n",
    "          return 'high'\n",
    "    if row['med_income_2012_13'] < med_income:\n",
    "          return 'low'\n",
    "\n",
    "#Apply function to create new dataframe coloumn\n",
    "ward_variables['income_category'] = ward_variables.apply(lambda row: income_category (row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import occupation data by ward\n",
    "\n",
    "#Import occupation .csv\n",
    "ward_occupation = pd.read_csv(\"Data/ward_occupation_data.csv\")\n",
    "\n",
    "#Calculate a new field to show the percentage of the employed population employed as professionals\n",
    "ward_occupation['combined_professionals_pct'] = ((ward_occupation['Sex: All persons; Occupation: 1. Managers, directors and senior officials; measures: Value'] +\n",
    "    ward_occupation['Sex: All persons; Occupation: 2. Professional occupations; measures: Value'])/\n",
    "    ward_occupation['Sex: All persons; Occupation: All categories: Occupation; measures: Value']) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Merge ward occupation information to be stored in the ward variables dataframe\n",
    "ward_variables = ward_variables.merge(ward_occupation[['geography code', 'combined_professionals_pct']], left_on='ward_code', right_on='geography code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Add colours for web mapping - define function\n",
    "\n",
    "#FUNCTION TO ASSIGN COLOURS (EQUAL INTERVAL)\n",
    "import numpy as np\n",
    "\n",
    "def color_assign (row, data, var, N, c):\n",
    "    \"\"\"\n",
    "    :param row: marker for apply function of dataframe\n",
    "    :param data: column of dataframe for calculations\n",
    "    :param var: column name\n",
    "    :param N: number of desired colour intervals\n",
    "    :param x: desired colour as tring either 'r' (red), 'g' (green), 'p'(purple) or 'b' (blue)\n",
    "    :return: <returns a colour accordining to the location of the value >returns the colour according to the interval the value belongs to\n",
    "    :example of application to a Selected column of a dataframe:\n",
    "    df['Color'] = df.apply(lambda row: color_assign (row, df['<Selected Column>'], '<Selected Column>'), axis=1)\n",
    "    \"\"\"\n",
    "    #CREATE A SPECTRUM OF COLOURS\n",
    "    increment = 255/(N+1)\n",
    "    RGB_tuples = []\n",
    "    \n",
    "    for x in range(N):\n",
    "\n",
    "        if c == 'r':\n",
    "            R = 255\n",
    "            G = (230-(x*increment))\n",
    "            B = (230-(x*increment))\n",
    "\n",
    "        if c == 'g':        \n",
    "            R = (230-(x*increment))\n",
    "            G = (255-(x/2*increment))\n",
    "            B = (230-(x*increment))\n",
    "\n",
    "        if c == 'p':        \n",
    "            R = (230-(x/4*increment))\n",
    "            G = (230-(x*increment))\n",
    "            B = 255\n",
    "            \n",
    "        if c == 'b':        \n",
    "            R = (230-(x*increment))\n",
    "            G = (230-(x*increment))\n",
    "            B = 255\n",
    "            \n",
    "        RGB_tuples.append((R,G,B))\n",
    "\n",
    "    #Convert to hex color format for mapping\n",
    "    hex_colour = []\n",
    "    for x in RGB_tuples:\n",
    "        hexa = '#%02x%02x%02x' % (x)\n",
    "        hex_colour.append(hexa)\n",
    "\n",
    "    minimum = data.min()\n",
    "    maximum = data.max()\n",
    "\n",
    "    N=len(hex_colour)\n",
    "    intervals = np.linspace(minimum, maximum, num=N+1)\n",
    "    for x in range(N):\n",
    "        if row[var] <= intervals[x+1]:\n",
    "            return hex_colour[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Assign color range for income\n",
    "ward_variables['income_color'] = ward_variables.apply(lambda row: color_assign(row, \n",
    "                                                ward_variables['med_income_2012_13'], 'med_income_2012_13', 7, 'b'), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Calculate number of each token within each ward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert token coordinates to shapely point file for each dataframe\n",
    "from shapely.geometry import Point\n",
    "\n",
    "token_points = {}\n",
    "token_points_coords = {}\n",
    "\n",
    "for x in tokens_lst:\n",
    "    #Make shapely points\n",
    "    token_points[x] = [Point(xy) for xy in zip(df[x]['eastings'], df[x]['northings'])]\n",
    "    \n",
    "    #convert shapely points into coordinate tuples\n",
    "    point_coords = []\n",
    "    for i in range(len(token_points[x])):\n",
    "        a = ([token_points[x][i].x, token_points[x][i].y])\n",
    "        point_coords.append(a)\n",
    "    token_points_coords[x] = point_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Build a spatial index based on the bounding boxes of the polygons\n",
    "from rtree import index\n",
    "idx = index.Index()\n",
    "count = -1\n",
    "for q in polygon_shapes:\n",
    "    count +=1\n",
    "    idx.insert(count, q.bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Assign one or more matching polygons to each point\n",
    "\n",
    "for x in tokens_lst:\n",
    "    matches = []\n",
    "\n",
    "    for i in range(len(df[x]['Token'])): #Iterate through each point\n",
    "        temp= None\n",
    "        #print \"Point \", i\n",
    "\n",
    "        #Iterate only through the bounding boxes which contain the point\n",
    "        for j in idx.intersection(token_points_coords[x][i]):\n",
    "            #Verify that point is within the polygon itself not just the bounding box\n",
    "            if token_points[x][i].within(polygons[j]):\n",
    "                temp=j\n",
    "                break\n",
    "        matches.append(temp) #Either the first match found, or None for no matches\n",
    "    \n",
    "    df[x]['ward_no'] = matches\n",
    "    \n",
    "    #Merge ward name and code information based on the polygons matched in the spatial join operation above\n",
    "    df[x] = df[x].merge(ward_variables[['position','ward_name', 'ward_code','n_s']], left_on='ward_no', right_on='position')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Names of wards\n",
    "london_wards = ward_variables['ward_name']\n",
    "\n",
    "#Count the number of tokens for each ward and join to ward_variables dataframe\n",
    "for x in tokens_lst:\n",
    "    token_count = []\n",
    "    for b in london_wards:\n",
    "            temp_df = df[x].loc[(df[x]['Token'] == x) & \n",
    "                                        (df[x]['ward_name'] == b)]\n",
    "            token_count.append(len(temp_df.index))\n",
    "    ward_variables[x + '_count'] = token_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Create KDE surface and attach KDE values to ward centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate surfances are used for wards north and south of the river to take into account the accessibility barrier effect of the thames. This ensures that a high concerntration of tokens along one side of the river does not effect calculations for the other side of the river."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#In order to record density surface for each token\n",
    "kernels = {}\n",
    "kernels_n = {}\n",
    "kernels_s = {}\n",
    "\n",
    "for x in tokens_lst:\n",
    "    \n",
    "    #Set up north south subsets\n",
    "    n_subset_df = df[x].loc[df[x]['n_s']=='n']\n",
    "    s_subset_df = df[x].loc[df[x]['n_s']=='s']\n",
    "\n",
    "    #Set up grid and KDE calculation\n",
    "    X, Y = np.mgrid[xmin:xmax:100j, ymin:ymax:100j]\n",
    "    positions = np.vstack([X.ravel(), Y.ravel()])\n",
    "    \n",
    "    #Calculate entiire density surface\n",
    "    values_coffee = np.vstack([df[x]['eastings'], df[x]['northings']])\n",
    "    kernels[x] = stats.gaussian_kde(values_coffee, 0.1) #Bandwidth set ot 0.1\n",
    "    Z = np.reshape(kernels[x](positions).T, X.shape)\n",
    "    \n",
    "    #Calculate north density surface\n",
    "    values_coffee_n = np.vstack([n_subset_df['eastings'], n_subset_df['northings']])\n",
    "    kernels_n[x] = stats.gaussian_kde(values_coffee_n, 0.1) #Bandwidth set ot 0.1\n",
    "\n",
    "    #Calculate south density surface\n",
    "    values_coffee_s = np.vstack([s_subset_df['eastings'], s_subset_df['northings']])\n",
    "    kernels_s[x] = stats.gaussian_kde(values_coffee_s, 0.1) #Bandwidth set ot 0.1\n",
    "    \n",
    "    #Plot KDE surface and save\n",
    "    token = x\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    fig = plt.figure(figsize=(12,8))\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.setp(ax.get_xticklabels(), fontsize=8)\n",
    "    plt.setp(ax.get_yticklabels(), fontsize=8)\n",
    "    \n",
    "    #Add london wards as patches\n",
    "    ax.add_collection(PatchCollection(ward_patches, alpha=1, facecolor='None', lw = 0.1, \n",
    "                                      edgecolor = '0'))\n",
    "    kde_surface = ax.imshow(np.rot90(Z), cmap=plt.cm.gist_earth_r,\n",
    "               extent=[xmin, xmax, ymin, ymax])\n",
    "    ax.plot(df[x]['eastings'], df[x]['northings'], 'k.', markersize=2)\n",
    "    ax.set_xlim([xmin, xmax])\n",
    "    ax.set_ylim([ymin, ymax])\n",
    "    plt.title(\"Kernel Density Surface (KDE) for Word Token '\" + token +\"'\", weight = 'heavy', y=1.05)\n",
    "    ax.arrow(560000, 158000, 0, 4000, head_width=700, head_length=700, fc='k', ec='k')\n",
    "\n",
    "    #ax.set_ylim([ymin, ymax])\n",
    "    plt.colorbar(kde_surface, cmap=plt.cm.gist_earth_r )\n",
    "\n",
    "    #plt.show()\n",
    "    fig.savefig('spatial_analysis/figures/KDE/' + token + '_kde.png', dpi=200, figsize = (12,8), transparent=True)\n",
    "\n",
    "    #Evaluate density at each ward centroid\n",
    "    \n",
    "    ward_variables[x + '_kde'] = \"\"\n",
    "    \n",
    "    for y in range(len(ward_variables['ward_name'])):\n",
    "        \n",
    "        #Use north density surface if ward is located north of the river\n",
    "        if ward_variables['n_s'].iloc[y] == 'n':\n",
    "            kde = kernels_n[x].evaluate([ward_variables['centroid'].iloc[y].x, \n",
    "                                 ward_variables['centroid'].iloc[y].y])\n",
    "            ward_variables.set_value(y, [x + '_kde'], kde)\n",
    "        \n",
    "        #Use south density surface if ward is located south of the river\n",
    "        if ward_variables['n_s'].iloc[y] == 's':\n",
    "            kde = kernels_s[x].evaluate([ward_variables['centroid'].iloc[y].x, \n",
    "                                     ward_variables['centroid'].iloc[y].y])\n",
    "            ward_variables.set_value(y, [x + '_kde'], kde)\n",
    "    \n",
    "    #Ensure output is type 'float' for later operations\n",
    "    ward_variables[x + '_kde'] = ward_variables[x + '_kde'].astype(float)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Calculate number of businesses within each ward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert business location coordinates to shapely point file for each dataframe\n",
    "from shapely.geometry import Point\n",
    "\n",
    "#Make shapely points\n",
    "food_bus_points = [Point(xy) for xy in zip(food_bus_df['eastings'], food_bus_df['northings'])]\n",
    "    \n",
    "#convert shapely points into coordinate tuples\n",
    "point_coords = []\n",
    "for i in range(len(food_bus_points)):\n",
    "    a = ([food_bus_points[i].x, food_bus_points[i].y])\n",
    "    point_coords.append(a)\n",
    "food_bus_coords = point_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Perform spatial join operation (using same bounding box index above to reduce the time)\n",
    "\n",
    "matches = []\n",
    "\n",
    "for i in range(len(food_bus_df['Latitude'])): #Iterate through each point\n",
    "    temp= None\n",
    "    #print \"Point \", i\n",
    "\n",
    "    #Iterate only through the bounding boxes which contain the point\n",
    "    for j in idx.intersection(food_bus_coords[i]):\n",
    "        #Verify that point is within the polygon itself not just the bounding box\n",
    "        if food_bus_points[i].within(polygons[j]):\n",
    "            temp=j\n",
    "            break\n",
    "    matches.append(temp) #Either the first match found, or None for no matches\n",
    "\n",
    "#Join ward information to the food business datframe based on the matching ward shapes\n",
    "food_bus_df['ward_no'] = matches\n",
    "food_bus_df = food_bus_df.merge(ward_variables[['position','ward_name', 'ward_code', 'n_s']], left_on='ward_no', right_on='position')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Calculate number of businesses within each ward by calculating the length of dataframe slices for each ward\n",
    "\n",
    "token_count = []\n",
    "\n",
    "for b in london_wards:\n",
    "        temp_df = food_bus_df.loc[food_bus_df['ward_name'] == b]\n",
    "        token_count.append(len(temp_df.index))\n",
    "        \n",
    "ward_variables['all_business_count'] = token_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Step 5 - Calculate location quotient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The location quotient is a double ratio measure with the ratio between the count for the word token against the count for all food businesses as a ratio between the local and global ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Calculate LQ for each token\n",
    "#Word token number for each ward centroid divided by all business number for each ward centroid divided by global aggregate\n",
    "\n",
    "for x in tokens_lst:\n",
    "    ward_variables[x +'_lq'] = (ward_variables[x + '_count']/(ward_variables['all_business_count']))/(len(df[x])/float(len(food_bus_df.index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Claire\\AppData\\Local\\Enthought\\Canopy\\User\\lib\\site-packages\\matplotlib\\pyplot.py:516: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "#Visualise the LQ\n",
    "\n",
    "for x in tokens_lst:\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    fig = plt.figure(figsize=(12,8))\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.setp(ax.get_xticklabels(), fontsize=8)\n",
    "    plt.setp(ax.get_yticklabels(), fontsize=8)\n",
    "\n",
    "    #Add london wards as patches\n",
    "    cmap = plt.get_cmap('Blues')\n",
    "    test = PatchCollection(ward_patches, alpha=1, facecolor=cmap(ward_variables[x + '_lq']), lw = 0.1, \n",
    "                                          edgecolor = '0')\n",
    "    ax.add_collection(test)\n",
    "    \n",
    "    #Add token locations as red points\n",
    "    ax.plot(df[x]['eastings'], df[x]['northings'], 'k.', markersize=4, markerfacecolor='red', alpha=0.8)\n",
    "    \n",
    "    #Titles and figure text settings\n",
    "    ax.set_xlim([xmin, xmax])\n",
    "    ax.set_ylim([ymin, ymax])\n",
    "    plt.title(\"Location Quotient (LQ) for Number of Word Token '\" + token +\"' by Ward\", weight = 'heavy', y=1.05)\n",
    "    ax.arrow(560000, 158000, 0, 4000, head_width=700, head_length=700, fc='k', ec='k')\n",
    "\n",
    "    #Add colourbar\n",
    "    m = cm.ScalarMappable(cmap=cm.Blues)\n",
    "    m.set_array(ward_variables[x + '_lq'])\n",
    "    plt.colorbar(m)\n",
    "    \n",
    "    #Save figures to new folder\n",
    "    fig.savefig('spatial_analysis/figures/LQ_token_no/' + x + '_LQ_no.png', dpi=200, figsize = (12,8), transparent=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 - Calculate smoothed location quotient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The smoothed location quotient has been calculated using the kernel density estimate evaluations performed above for each ward centroid rather than a count of businesses in each ward. This measure smooths out the arbitrary effect of ward boundaries. \n",
    "\n",
    "The location quotient is a double ratio measure with the ratio between the KDE for the word token against the KDE for all food businesses as a ratio between the local and global ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Calculate the ward centroid KDE for all businesses in order to complete operation\n",
    "\n",
    "#Calculate kde for all wards\n",
    "n_subset_df = food_bus_df.loc[food_bus_df['n_s']=='n']\n",
    "s_subset_df = food_bus_df.loc[food_bus_df['n_s']=='s']\n",
    "\n",
    "#Add values for LQ for all businesses by ward\n",
    "#Set up grid and KDE calculation\n",
    "X, Y = np.mgrid[xmin:xmax:100j, ymin:ymax:100j]\n",
    "positions = np.vstack([X.ravel(), Y.ravel()])\n",
    "values = np.vstack([food_bus_df['eastings'], food_bus_df['northings']])\n",
    "kernels = stats.gaussian_kde(values, 0.1) #Bandwidth set ot 0.1\n",
    "Z = np.reshape(kernels(positions).T, X.shape)\n",
    "\n",
    "#Calculate north density surface\n",
    "values_coffee_n = np.vstack([n_subset_df['eastings'], n_subset_df['northings']])\n",
    "kernels_n = stats.gaussian_kde(values_coffee_n, 0.1) #Bandwidth set ot 0.1\n",
    "\n",
    "#Calculate north density surface\n",
    "values_coffee_s = np.vstack([s_subset_df['eastings'], s_subset_df['northings']])\n",
    "kernels_s = stats.gaussian_kde(values_coffee_s, 0.1) #Bandwidth set ot 0.1\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "ax = fig.add_subplot(111)\n",
    "plt.setp(ax.get_xticklabels(), fontsize=8)\n",
    "plt.setp(ax.get_yticklabels(), fontsize=8)\n",
    "\n",
    "#Add london wards as patches\n",
    "ax.add_collection(PatchCollection(ward_patches, alpha=1, facecolor='None', lw = 0.1, \n",
    "                                      edgecolor = '0'))\n",
    "kde_surface = ax.imshow(np.rot90(Z), cmap=plt.cm.gist_earth_r,\n",
    "               extent=[xmin, xmax, ymin, ymax])\n",
    "ax.plot(food_bus_df['eastings'], food_bus_df['northings'], 'k.', markersize=2)\n",
    "ax.set_xlim([xmin, xmax])\n",
    "ax.set_ylim([ymin, ymax])\n",
    "plt.title(\"Kernel Density Surface (KDE) for All Food Businesses\", weight = 'heavy', y=1.05)\n",
    "ax.arrow(560000, 158000, 0, 4000, head_width=700, head_length=700, fc='k', ec='k')\n",
    "\n",
    "#ax.set_ylim([ymin, ymax])\n",
    "plt.colorbar(kde_surface, cmap=plt.cm.gist_earth_r )\n",
    "\n",
    "#plt.show()\n",
    "fig.savefig('spatial_analysis/figures/KDE/allfoodbusiness_kde.png', dpi=200, figsize = (12,8), transparent=True)\n",
    "\n",
    "#Evaluate density at each ward centroid\n",
    "ward_kde = []\n",
    "    \n",
    "for y in range(len(ward_variables['ward_name'])):\n",
    "    kde = kernels.evaluate([ward_variables['centroid'].iloc[y].x, \n",
    "                             ward_variables['centroid'].iloc[y].y])\n",
    "    ward_kde.append(kde[0])\n",
    "        \n",
    "ward_variables['all_business_kde'] = \"\"\n",
    "\n",
    "for y in range(len(ward_variables['ward_name'])):\n",
    "     #Use north density surface if ward is located north of the river\n",
    "    if ward_variables['n_s'].iloc[y] == 'n':\n",
    "        kde = kernels_n.evaluate([ward_variables['centroid'].iloc[y].x, \n",
    "                                 ward_variables['centroid'].iloc[y].y])\n",
    "        ward_variables.set_value(y, ['all_business_kde'], kde)\n",
    "        \n",
    "    #Use south density surface if ward is located south of the river\n",
    "    if ward_variables['n_s'].iloc[y] == 's':\n",
    "        kde = kernels_s.evaluate([ward_variables['centroid'].iloc[y].x, \n",
    "                                     ward_variables['centroid'].iloc[y].y])\n",
    "        ward_variables.set_value(y, ['all_business_kde'], kde)\n",
    "\n",
    "ward_variables['all_business_kde'] = ward_variables['all_business_kde'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Calculate the smooth location quotient for each ward based on centroid KDEs\n",
    "for x in tokens_lst:\n",
    "    ward_variables[x +'_smooth_lq'] = (ward_variables[x + '_kde']/(ward_variables['all_business_kde']))/(sum(ward_variables[x + '_kde'])/float(sum(ward_variables['all_business_kde'])))\n",
    "    #Convert to float\n",
    "    ward_variables[x +'_smooth_lq'] = ward_variables[x +'_smooth_lq'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Calculate colour fields (green) for location quotients using function defined above\n",
    "for x in tokens_lst:\n",
    "    ward_variables[x +'_smooth_lq_color'] = ward_variables.apply(lambda row: color_assign(row, \n",
    "                                                    ward_variables[x +'_smooth_lq'], x +'_smooth_lq', 7, 'g'), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Visualise the Smooth LQ\n",
    "\n",
    "for x in tokens_lst:\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    fig = plt.figure(figsize=(12,8))\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.setp(ax.get_xticklabels(), fontsize=8)\n",
    "    plt.setp(ax.get_yticklabels(), fontsize=8)\n",
    "\n",
    "\n",
    "    #Add london wards as patches\n",
    "    cmap = plt.get_cmap('BuGn')\n",
    "    test = PatchCollection(ward_patches, alpha=1, facecolor=cmap(ward_variables[x + '_smooth_lq']), lw = 0.1, \n",
    "                                          edgecolor = '0')\n",
    "    ax.add_collection(test)\n",
    "    \n",
    "    #Plot points\n",
    "    ax.plot(df[x]['eastings'], df[x]['northings'], 'k.', markersize=4, markerfacecolor='red', alpha=0.8)\n",
    "    \n",
    "    #Axis options\n",
    "    ax.set_xlim([xmin, xmax])\n",
    "    ax.set_ylim([ymin, ymax])\n",
    "    plt.title(\"KDE Location Quotient (LQ) for Word Token '\" + x +\"'\", weight = 'heavy', y=1.05)\n",
    "    ax.arrow(560000, 158000, 0, 4000, head_width=700, head_length=700, fc='k', ec='k')\n",
    "\n",
    "    #Add Color bar\n",
    "    m = cm.ScalarMappable(cmap=cm.BuGn)\n",
    "    m.set_array(ward_variables[x + '_smooth_lq'])\n",
    "    plt.colorbar(m)\n",
    "    \n",
    "    #Save figure\n",
    "    fig.savefig('spatial_analysis/figures/LQ_KDE_smooth/' + x + '_smooth_LQ.png', dpi=200, figsize = (12,8), transparent = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 - Moran's I Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Global Moran's I\n",
    "\n",
    "import pysal\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#Create rooks weights matrix\n",
    "w = pysal.rook_from_shapefile(\"Data/ESRI/london_wards.shp\")\n",
    "\n",
    "morans_value = []\n",
    "morans_EI = []\n",
    "morans_p = []\n",
    "\n",
    "for x in tokens_lst:\n",
    "    #Import variable as array\n",
    "    y = np.array(ward_variables[x + '_kde'].astype(float))\n",
    "\n",
    "    #Calculate Moran's I\n",
    "    mi = pysal.Moran(y, w, two_tailed=False)\n",
    "\n",
    "    \n",
    "    morans_value.append(\"%.3f\"%mi.I)\n",
    "    morans_EI.append(mi.EI)\n",
    "    morans_p.append(\"%.5f\"%mi.p_norm)\n",
    "\n",
    "global_stats_df = pd.DataFrame({'morans_value': morans_value, 'morans_EI': morans_EI, 'morans_p':morans_p}, \n",
    "                                index=tokens_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>morans_EI</th>\n",
       "      <th>morans_p</th>\n",
       "      <th>morans_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cafe</th>\n",
       "      <td>-0.001603</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coffee</th>\n",
       "      <td>-0.001603</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pizza</th>\n",
       "      <td>-0.001603</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wine</th>\n",
       "      <td>-0.001603</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sushi</th>\n",
       "      <td>-0.001603</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thai</th>\n",
       "      <td>-0.001603</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chicken</th>\n",
       "      <td>-0.001603</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fried</th>\n",
       "      <td>-0.001603</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fish</th>\n",
       "      <td>-0.001603</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kebab</th>\n",
       "      <td>-0.001603</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>costcutter</th>\n",
       "      <td>-0.001603</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>waitrose</th>\n",
       "      <td>-0.001603</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sainsburys</th>\n",
       "      <td>-0.001603</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tesco</th>\n",
       "      <td>-0.001603</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grill</th>\n",
       "      <td>-0.001603</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.578</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            morans_EI morans_p morans_value\n",
       "cafe        -0.001603  0.00000        0.753\n",
       "coffee      -0.001603  0.00000        0.681\n",
       "pizza       -0.001603  0.00000        0.547\n",
       "wine        -0.001603  0.00000        0.625\n",
       "sushi       -0.001603  0.00000        0.437\n",
       "thai        -0.001603  0.00000        0.537\n",
       "chicken     -0.001603  0.00000        0.655\n",
       "fried       -0.001603  0.00000        0.592\n",
       "fish        -0.001603  0.00000        0.638\n",
       "kebab       -0.001603  0.00000        0.535\n",
       "costcutter  -0.001603  0.00000        0.443\n",
       "waitrose    -0.001603  0.00000        0.538\n",
       "sainsburys  -0.001603  0.00000        0.329\n",
       "tesco       -0.001603  0.00000        0.626\n",
       "grill       -0.001603  0.00000        0.578"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Local Moran's I\n",
    "\n",
    "for x in tokens_lst:\n",
    "\n",
    "    y = np.array(ward_variables[x + '_kde'])\n",
    "    lm = pysal.Moran_Local(y,w)\n",
    "    \n",
    "    ward_variables[x + '_lmoran_value'] = lm.Is\n",
    "    ward_variables[x + '_lmoran_p'] = lm.p_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Add colours for web mapping of morans I value (red)\n",
    "for x in tokens_lst:\n",
    "    ward_variables[x +'_morans_color'] = ward_variables.apply(lambda row: color_assign(row, \n",
    "                                                    ward_variables[x + '_lmoran_value'], x + '_lmoran_value', 7, 'r'), axis=1)\n",
    "\n",
    "#but color grey if not significant\n",
    "for x in tokens_lst:\n",
    "    for i in range(len(ward_variables[x +'_morans_color'])):\n",
    "        if ward_variables[x + '_lmoran_p'].loc[i] > 0.05:\n",
    "            ward_variables.set_value(i, [x + '_morans_color'], '#d3d3d3') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Visualise Local Morans Values\n",
    "\n",
    "for x in tokens_lst:\n",
    "    fig = plt.figure(figsize=(12,8))\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.setp(ax.get_xticklabels(), fontsize=8)\n",
    "    plt.setp(ax.get_yticklabels(), fontsize=8)\n",
    "    \n",
    "    cmap = plt.get_cmap('Purples')\n",
    "    test = PatchCollection(ward_patches, alpha=1, facecolor=cmap(ward_variables[x + '_lmoran_value']), lw = 0.1, \n",
    "                                          edgecolor = '0')\n",
    "    ax.add_collection(test)\n",
    "\n",
    "    ax.plot(df[x]['eastings'], df[x]['northings'], 'k.', markersize=4, markerfacecolor='green', alpha=0.8)\n",
    "    ax.set_xlim([xmin, xmax])\n",
    "    ax.set_ylim([ymin, ymax])\n",
    "    plt.title(\"Local Morans I Values for Word Token '\" + x +\"'\", weight = 'heavy', y=1.05)\n",
    "    ax.arrow(560000, 158000, 0, 4000, head_width=700, head_length=700, fc='k', ec='k')\n",
    "\n",
    "    m = cm.ScalarMappable(cmap=cm.Purples)\n",
    "    m.set_array(ward_variables[x + '_lmoran_value'])\n",
    "    plt.colorbar(m)\n",
    "\n",
    "    fig.savefig('spatial_analysis/figures/local_morans/' + x + '_local_morans.png', dpi=200, figsize = (12,8), transparent = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Visualise Local Morans P Values\n",
    "\n",
    "for x in tokens_lst:\n",
    "    \n",
    "    fig = plt.figure(figsize=(12,8))\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.setp(ax.get_xticklabels(), fontsize=8)\n",
    "    plt.setp(ax.get_yticklabels(), fontsize=8)\n",
    "    \n",
    "    cmap = plt.get_cmap('bwr')\n",
    "    test = PatchCollection(ward_patches, alpha=1, facecolor=cmap(ward_variables[x + '_lmoran_p']), lw = 0.1, \n",
    "                                          edgecolor = '0')\n",
    "    ax.add_collection(test)\n",
    "\n",
    "    ax.plot(df[x]['eastings'], df[x]['northings'], 'k.', markersize=4, markerfacecolor='green', alpha=0.8)\n",
    "    ax.set_xlim([xmin, xmax])\n",
    "    ax.set_ylim([ymin, ymax])\n",
    "    plt.title(\"Local Morans I P Values for Word Token '\" + x +\"'\", weight = 'heavy', y=1.05)\n",
    "    ax.arrow(560000, 158000, 0, 4000, head_width=700, head_length=700, fc='k', ec='k')\n",
    "\n",
    "    m = cm.ScalarMappable(cmap=cm.bwr)\n",
    "    m.set_array(ward_variables[x + '_lmoran_p'])\n",
    "    plt.colorbar(m)\n",
    "\n",
    "    fig.savefig('spatial_analysis/figures/local_morans/' + x + '_local_morans_p.png', dpi=200, figsize = (12,8), transparent=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7 - Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Claire\\AppData\\Local\\Enthought\\Canopy\\User\\lib\\site-packages\\pandas\\core\\frame.py:2748: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  **kwargs)\n"
     ]
    }
   ],
   "source": [
    "#Create scatter Matrix of Correlations\n",
    "\n",
    "#Subset Data\n",
    "lst = ['med_income_2012_13', 'combined_professionals_pct']\n",
    "\n",
    "subset_columns = []\n",
    "\n",
    "for x in lst:\n",
    "    subset_columns.append(x)\n",
    "\n",
    "for x in tokens_lst:\n",
    "    a = x + '_smooth_lq'\n",
    "    subset_columns.append(a)\n",
    "    \n",
    "ward_variables_subset = ward_variables[subset_columns]\n",
    "\n",
    "#Rename columns for visualisation\n",
    "for x in tokens_lst:\n",
    "    #Rename columns for visualisation\n",
    "    ward_variables_subset.rename(columns={x + '_smooth_lq': x}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# takes the dataset, an alpha value for opacity, a figure size setting, and a specification of the diagonal charts\n",
    "\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "fig = plt.figure(figsize=(13,13))\n",
    "a = pd.scatter_matrix(ward_variables_subset, alpha=0.2, diagonal='kde', figsize=(12,12))\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Calculate correlation coefficients\n",
    "a = ward_variables_subset.corr()\n",
    "subset = a.iloc[2:16]\n",
    "\n",
    "global_stats_df['income_correlation'] = subset['med_income_2012_13']\n",
    "global_stats_df['professional_pct_correlation'] = subset['combined_professionals_pct']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>morans_EI</th>\n",
       "      <th>morans_p</th>\n",
       "      <th>morans_value</th>\n",
       "      <th>income_correlation</th>\n",
       "      <th>professional_pct_correlation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cafe</th>\n",
       "      <td>-0.001603</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.753</td>\n",
       "      <td>0.002738</td>\n",
       "      <td>0.088544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coffee</th>\n",
       "      <td>-0.001603</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.681</td>\n",
       "      <td>0.300065</td>\n",
       "      <td>0.351823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pizza</th>\n",
       "      <td>-0.001603</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.547</td>\n",
       "      <td>-0.142556</td>\n",
       "      <td>-0.142460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wine</th>\n",
       "      <td>-0.001603</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.625</td>\n",
       "      <td>-0.097631</td>\n",
       "      <td>-0.083396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sushi</th>\n",
       "      <td>-0.001603</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.437</td>\n",
       "      <td>0.258518</td>\n",
       "      <td>0.315021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thai</th>\n",
       "      <td>-0.001603</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.537</td>\n",
       "      <td>0.433978</td>\n",
       "      <td>0.479849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chicken</th>\n",
       "      <td>-0.001603</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.655</td>\n",
       "      <td>-0.469419</td>\n",
       "      <td>-0.440650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fried</th>\n",
       "      <td>-0.001603</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.592</td>\n",
       "      <td>-0.328198</td>\n",
       "      <td>-0.274218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fish</th>\n",
       "      <td>-0.001603</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.638</td>\n",
       "      <td>-0.288894</td>\n",
       "      <td>-0.400196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kebab</th>\n",
       "      <td>-0.001603</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.535</td>\n",
       "      <td>-0.335732</td>\n",
       "      <td>-0.371847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>costcutter</th>\n",
       "      <td>-0.001603</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.443</td>\n",
       "      <td>-0.153771</td>\n",
       "      <td>-0.211807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>waitrose</th>\n",
       "      <td>-0.001603</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.538</td>\n",
       "      <td>0.334962</td>\n",
       "      <td>0.326170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sainsburys</th>\n",
       "      <td>-0.001603</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.329</td>\n",
       "      <td>0.026971</td>\n",
       "      <td>0.011479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tesco</th>\n",
       "      <td>-0.001603</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.626</td>\n",
       "      <td>-0.062816</td>\n",
       "      <td>-0.078366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grill</th>\n",
       "      <td>-0.001603</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.578</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            morans_EI morans_p morans_value  income_correlation  \\\n",
       "cafe        -0.001603  0.00000        0.753            0.002738   \n",
       "coffee      -0.001603  0.00000        0.681            0.300065   \n",
       "pizza       -0.001603  0.00000        0.547           -0.142556   \n",
       "wine        -0.001603  0.00000        0.625           -0.097631   \n",
       "sushi       -0.001603  0.00000        0.437            0.258518   \n",
       "thai        -0.001603  0.00000        0.537            0.433978   \n",
       "chicken     -0.001603  0.00000        0.655           -0.469419   \n",
       "fried       -0.001603  0.00000        0.592           -0.328198   \n",
       "fish        -0.001603  0.00000        0.638           -0.288894   \n",
       "kebab       -0.001603  0.00000        0.535           -0.335732   \n",
       "costcutter  -0.001603  0.00000        0.443           -0.153771   \n",
       "waitrose    -0.001603  0.00000        0.538            0.334962   \n",
       "sainsburys  -0.001603  0.00000        0.329            0.026971   \n",
       "tesco       -0.001603  0.00000        0.626           -0.062816   \n",
       "grill       -0.001603  0.00000        0.578                 NaN   \n",
       "\n",
       "            professional_pct_correlation  \n",
       "cafe                            0.088544  \n",
       "coffee                          0.351823  \n",
       "pizza                          -0.142460  \n",
       "wine                           -0.083396  \n",
       "sushi                           0.315021  \n",
       "thai                            0.479849  \n",
       "chicken                        -0.440650  \n",
       "fried                          -0.274218  \n",
       "fish                           -0.400196  \n",
       "kebab                          -0.371847  \n",
       "costcutter                     -0.211807  \n",
       "waitrose                        0.326170  \n",
       "sainsburys                      0.011479  \n",
       "tesco                          -0.078366  \n",
       "grill                                NaN  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_stats_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8 - Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Claire\\AppData\\Local\\Enthought\\Canopy\\User\\lib\\site-packages\\pysal\\spreg\\user_output.py:337: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n",
      "  if i == None:\n"
     ]
    }
   ],
   "source": [
    "#Regression - Income only\n",
    "\n",
    "w = pysal.rook_from_shapefile(\"Data/ESRI/london_wards.shp\")\n",
    "x = np.reshape(ward_variables['med_income_2012_13'], newshape=(625, 1))\n",
    "\n",
    "var = 'income'\n",
    "\n",
    "r2 = []\n",
    "ar2 = []\n",
    "moran_res = []\n",
    "coefficient = []\n",
    "t_stat = []\n",
    "\n",
    "for i in tokens_lst:\n",
    "    y = np.reshape(a=ward_variables[i + '_smooth_lq'],newshape=(625,1))\n",
    "    a = pysal.spreg.ols.OLS(y,x,w, spat_diag=True, moran=True)\n",
    "    ward_variables[i + '_' + var + '_residuals'] = a.u\n",
    "    \n",
    "    r2.append(a.r2)\n",
    "    ar2.append(a.ar2)\n",
    "    moran_res.append(a.moran_res)\n",
    "    coefficient.append(a.betas)\n",
    "    t_stat.append(a.t_stat)\n",
    "\n",
    "global_stats_df[var + '_r2'] = r2\n",
    "global_stats_df[var + '_ar2'] = ar2\n",
    "global_stats_df[var + '_moran_res'] = moran_res\n",
    "global_stats_df[var + '_coefficient'] = coefficient\n",
    "global_stats_df[var + '_t_stat'] = t_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Visualise income regression residuals\n",
    "\n",
    "for x in tokens_lst:\n",
    "    \n",
    "    fig = plt.figure(figsize=(12,8))\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.setp(ax.get_xticklabels(), fontsize=8)\n",
    "    plt.setp(ax.get_yticklabels(), fontsize=8)\n",
    "    \n",
    "    cmap = plt.get_cmap('bwr')\n",
    "    test = PatchCollection(ward_patches, alpha=1, facecolor=cmap(ward_variables[x + '_income_residuals']), lw = 0.1, \n",
    "                                          edgecolor = '0')\n",
    "    ax.add_collection(test)\n",
    "\n",
    "    ax.plot(df[x]['eastings'], df[x]['northings'], 'k.', markersize=4, markerfacecolor='green', alpha=0.8)\n",
    "    ax.set_xlim([xmin, xmax])\n",
    "    ax.set_ylim([ymin, ymax])\n",
    "    plt.title(\"Local Morans I P Values for Word Token '\" + x +\"'\", weight = 'heavy', y=1.05)\n",
    "    ax.arrow(560000, 158000, 0, 4000, head_width=700, head_length=700, fc='k', ec='k')\n",
    "\n",
    "    m = cm.ScalarMappable(cmap=cm.bwr)\n",
    "    m.set_array(ward_variables[x + '_income_residuals'])\n",
    "    plt.colorbar(m)\n",
    "\n",
    "    fig.savefig('spatial_analysis/figures/regression_residuals/' + x + '_income_residuals.png', dpi=200, figsize = (12,8), transparent=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Regression - proportion of population professional only\n",
    "\n",
    "w = pysal.rook_from_shapefile(\"Data/ESRI/london_wards.shp\")\n",
    "x = np.reshape(ward_variables['combined_professionals_pct'], newshape=(625, 1))\n",
    "\n",
    "var = 'professionals_pct'\n",
    "\n",
    "r2 = []\n",
    "ar2 = []\n",
    "moran_res = []\n",
    "coefficient = []\n",
    "t_stat = []\n",
    "\n",
    "for i in tokens_lst:\n",
    "    y = np.reshape(a=ward_variables[i + '_smooth_lq'],newshape=(625,1))\n",
    "    a = pysal.spreg.ols.OLS(y,x,w, spat_diag=True, moran=True)\n",
    "    ward_variables[i + '_' + var + '_residuals'] = a.u\n",
    "    \n",
    "    r2.append(a.r2)\n",
    "    ar2.append(a.ar2)\n",
    "    moran_res.append(a.moran_res)\n",
    "    coefficient.append(a.betas)\n",
    "    t_stat.append(a.t_stat)\n",
    "\n",
    "global_stats_df[var + '_r2'] = r2\n",
    "global_stats_df[var + '_ar2'] = ar2\n",
    "global_stats_df[var + '_moran_res'] = moran_res\n",
    "global_stats_df[var + '_coefficient'] = coefficient\n",
    "global_stats_df[var + '_t_stat'] = t_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Regression - proportion of population professional and income\n",
    "\n",
    "w = pysal.rook_from_shapefile(\"Data/ESRI/london_wards.shp\")\n",
    "x = ward_variables[['med_income_2012_13', 'combined_professionals_pct']].as_matrix()\n",
    "\n",
    "var = 'income_professionals'\n",
    "\n",
    "r2 = []\n",
    "ar2 = []\n",
    "moran_res = []\n",
    "coefficient = []\n",
    "t_stat = []\n",
    "\n",
    "for i in tokens_lst:\n",
    "    y = np.reshape(a=ward_variables[i + '_smooth_lq'],newshape=(625,1))\n",
    "    a = pysal.spreg.ols.OLS(y,x,w, spat_diag=True, moran=True)\n",
    "    ward_variables[i + '_' + var + '_residuals'] = a.u\n",
    "    \n",
    "    r2.append(a.r2)\n",
    "    ar2.append(a.ar2)\n",
    "    moran_res.append(a.moran_res)\n",
    "    coefficient.append(a.betas)\n",
    "    t_stat.append(a.t_stat)\n",
    "\n",
    "global_stats_df[var + '_r2'] = r2\n",
    "global_stats_df[var + '_ar2'] = ar2\n",
    "global_stats_df[var + '_moran_res'] = moran_res\n",
    "global_stats_df[var + '_coefficient'] = coefficient\n",
    "global_stats_df[var + '_t_stat'] = t_stat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9 - Explore Clustering Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['position',\n",
       " 'ward_code',\n",
       " 'ward_name',\n",
       " 'code',\n",
       " 'n_s',\n",
       " 'centroid',\n",
       " 'centroid_x',\n",
       " 'centroid_y',\n",
       " 'Code',\n",
       " 'med_income_2012_13',\n",
       " 'income_category',\n",
       " 'geography code',\n",
       " 'combined_professionals_pct',\n",
       " 'income_color',\n",
       " 'cafe_count',\n",
       " 'coffee_count',\n",
       " 'pizza_count',\n",
       " 'wine_count',\n",
       " 'sushi_count',\n",
       " 'thai_count',\n",
       " 'chicken_count',\n",
       " 'fried_count',\n",
       " 'fish_count',\n",
       " 'kebab_count',\n",
       " 'costcutter_count',\n",
       " 'waitrose_count',\n",
       " 'sainsburys_count',\n",
       " 'tesco_count',\n",
       " 'grill_count',\n",
       " 'cafe_kde',\n",
       " 'coffee_kde',\n",
       " 'pizza_kde',\n",
       " 'wine_kde',\n",
       " 'sushi_kde',\n",
       " 'thai_kde',\n",
       " 'chicken_kde',\n",
       " 'fried_kde',\n",
       " 'fish_kde',\n",
       " 'kebab_kde',\n",
       " 'costcutter_kde',\n",
       " 'waitrose_kde',\n",
       " 'sainsburys_kde',\n",
       " 'tesco_kde',\n",
       " 'grill_kde',\n",
       " 'all_business_count',\n",
       " 'cafe_lq',\n",
       " 'coffee_lq',\n",
       " 'pizza_lq',\n",
       " 'wine_lq',\n",
       " 'sushi_lq',\n",
       " 'thai_lq',\n",
       " 'chicken_lq',\n",
       " 'fried_lq',\n",
       " 'fish_lq',\n",
       " 'kebab_lq',\n",
       " 'costcutter_lq',\n",
       " 'waitrose_lq',\n",
       " 'sainsburys_lq',\n",
       " 'tesco_lq',\n",
       " 'grill_lq',\n",
       " 'all_business_kde',\n",
       " 'cafe_smooth_lq',\n",
       " 'coffee_smooth_lq',\n",
       " 'pizza_smooth_lq',\n",
       " 'wine_smooth_lq',\n",
       " 'sushi_smooth_lq',\n",
       " 'thai_smooth_lq',\n",
       " 'chicken_smooth_lq',\n",
       " 'fried_smooth_lq',\n",
       " 'fish_smooth_lq',\n",
       " 'kebab_smooth_lq',\n",
       " 'costcutter_smooth_lq',\n",
       " 'waitrose_smooth_lq',\n",
       " 'sainsburys_smooth_lq',\n",
       " 'tesco_smooth_lq',\n",
       " 'grill_smooth_lq',\n",
       " 'cafe_smooth_lq_color',\n",
       " 'coffee_smooth_lq_color',\n",
       " 'pizza_smooth_lq_color',\n",
       " 'wine_smooth_lq_color',\n",
       " 'sushi_smooth_lq_color',\n",
       " 'thai_smooth_lq_color',\n",
       " 'chicken_smooth_lq_color',\n",
       " 'fried_smooth_lq_color',\n",
       " 'fish_smooth_lq_color',\n",
       " 'kebab_smooth_lq_color',\n",
       " 'costcutter_smooth_lq_color',\n",
       " 'waitrose_smooth_lq_color',\n",
       " 'sainsburys_smooth_lq_color',\n",
       " 'tesco_smooth_lq_color',\n",
       " 'grill_smooth_lq_color',\n",
       " 'cafe_lmoran_value',\n",
       " 'cafe_lmoran_p',\n",
       " 'coffee_lmoran_value',\n",
       " 'coffee_lmoran_p',\n",
       " 'pizza_lmoran_value',\n",
       " 'pizza_lmoran_p',\n",
       " 'wine_lmoran_value',\n",
       " 'wine_lmoran_p',\n",
       " 'sushi_lmoran_value',\n",
       " 'sushi_lmoran_p',\n",
       " 'thai_lmoran_value',\n",
       " 'thai_lmoran_p',\n",
       " 'chicken_lmoran_value',\n",
       " 'chicken_lmoran_p',\n",
       " 'fried_lmoran_value',\n",
       " 'fried_lmoran_p',\n",
       " 'fish_lmoran_value',\n",
       " 'fish_lmoran_p',\n",
       " 'kebab_lmoran_value',\n",
       " 'kebab_lmoran_p',\n",
       " 'costcutter_lmoran_value',\n",
       " 'costcutter_lmoran_p',\n",
       " 'waitrose_lmoran_value',\n",
       " 'waitrose_lmoran_p',\n",
       " 'sainsburys_lmoran_value',\n",
       " 'sainsburys_lmoran_p',\n",
       " 'tesco_lmoran_value',\n",
       " 'tesco_lmoran_p',\n",
       " 'grill_lmoran_value',\n",
       " 'grill_lmoran_p',\n",
       " 'cafe_morans_color',\n",
       " 'coffee_morans_color',\n",
       " 'pizza_morans_color',\n",
       " 'wine_morans_color',\n",
       " 'sushi_morans_color',\n",
       " 'thai_morans_color',\n",
       " 'chicken_morans_color',\n",
       " 'fried_morans_color',\n",
       " 'fish_morans_color',\n",
       " 'kebab_morans_color',\n",
       " 'costcutter_morans_color',\n",
       " 'waitrose_morans_color',\n",
       " 'sainsburys_morans_color',\n",
       " 'tesco_morans_color',\n",
       " 'grill_morans_color',\n",
       " 'cafe_income_residuals',\n",
       " 'coffee_income_residuals',\n",
       " 'pizza_income_residuals',\n",
       " 'wine_income_residuals',\n",
       " 'sushi_income_residuals',\n",
       " 'thai_income_residuals',\n",
       " 'chicken_income_residuals',\n",
       " 'fried_income_residuals',\n",
       " 'fish_income_residuals',\n",
       " 'kebab_income_residuals',\n",
       " 'costcutter_income_residuals',\n",
       " 'waitrose_income_residuals',\n",
       " 'sainsburys_income_residuals',\n",
       " 'tesco_income_residuals',\n",
       " 'grill_income_residuals',\n",
       " 'cafe_professionals_pct_residuals',\n",
       " 'coffee_professionals_pct_residuals',\n",
       " 'pizza_professionals_pct_residuals',\n",
       " 'wine_professionals_pct_residuals',\n",
       " 'sushi_professionals_pct_residuals',\n",
       " 'thai_professionals_pct_residuals',\n",
       " 'chicken_professionals_pct_residuals',\n",
       " 'fried_professionals_pct_residuals',\n",
       " 'fish_professionals_pct_residuals',\n",
       " 'kebab_professionals_pct_residuals',\n",
       " 'costcutter_professionals_pct_residuals',\n",
       " 'waitrose_professionals_pct_residuals',\n",
       " 'sainsburys_professionals_pct_residuals',\n",
       " 'tesco_professionals_pct_residuals',\n",
       " 'grill_professionals_pct_residuals',\n",
       " 'cafe_income_professionals_residuals',\n",
       " 'coffee_income_professionals_residuals',\n",
       " 'pizza_income_professionals_residuals',\n",
       " 'wine_income_professionals_residuals',\n",
       " 'sushi_income_professionals_residuals',\n",
       " 'thai_income_professionals_residuals',\n",
       " 'chicken_income_professionals_residuals',\n",
       " 'fried_income_professionals_residuals',\n",
       " 'fish_income_professionals_residuals',\n",
       " 'kebab_income_professionals_residuals',\n",
       " 'costcutter_income_professionals_residuals',\n",
       " 'waitrose_income_professionals_residuals',\n",
       " 'sainsburys_income_professionals_residuals',\n",
       " 'tesco_income_professionals_residuals',\n",
       " 'grill_income_professionals_residuals',\n",
       " 'ag_clustersincome_coffee0',\n",
       " 'ag_clustersincome_coffee1',\n",
       " 'ag_clustersincome_coffee2',\n",
       " 'ag_clustersincome_coffee3',\n",
       " 'ag_clustersincome_coffee4',\n",
       " 'ag_clustersincome_coffee5',\n",
       " 'ag_clustersincome_coffee6',\n",
       " 'ag_clustersincome_chicken0',\n",
       " 'ag_clustersincome_chicken1',\n",
       " 'ag_clustersincome_chicken2',\n",
       " 'ag_clustersincome_chicken3',\n",
       " 'ag_clustersincome_chicken4',\n",
       " 'ag_clustersincome_chicken5',\n",
       " 'ag_clustersincome_chicken6',\n",
       " 'ag_clusterscoffee_chicken0',\n",
       " 'ag_clusterscoffee_chicken1',\n",
       " 'ag_clusterscoffee_chicken2',\n",
       " 'ag_clusterscoffee_chicken3',\n",
       " 'ag_clusterscoffee_chicken4',\n",
       " 'ag_clusterscoffee_chicken5',\n",
       " 'ag_clusterscoffee_chicken6',\n",
       " 'ag_clustersincome_coffee_chicken0',\n",
       " 'ag_clustersincome_coffee_chicken1',\n",
       " 'ag_clustersincome_coffee_chicken2',\n",
       " 'ag_clustersincome_coffee_chicken3',\n",
       " 'ag_clustersincome_coffee_chicken4',\n",
       " 'ag_clustersincome_coffee_chicken5',\n",
       " 'ag_clustersincome_coffee_chicken6',\n",
       " 'ag_clustersincome_coffee_chicken_thai_kebab0',\n",
       " 'ag_clustersincome_coffee_chicken_thai_kebab1',\n",
       " 'ag_clustersincome_coffee_chicken_thai_kebab2',\n",
       " 'ag_clustersincome_coffee_chicken_thai_kebab3',\n",
       " 'ag_clustersincome_coffee_chicken_thai_kebab4',\n",
       " 'ag_clustersincome_coffee_chicken_thai_kebab5',\n",
       " 'ag_clustersincome_coffee_chicken_thai_kebab6',\n",
       " 'ag_clusterscoffee_chicken_thai_kebab0',\n",
       " 'ag_clusterscoffee_chicken_thai_kebab1',\n",
       " 'ag_clusterscoffee_chicken_thai_kebab2',\n",
       " 'ag_clusterscoffee_chicken_thai_kebab3',\n",
       " 'ag_clusterscoffee_chicken_thai_kebab4',\n",
       " 'ag_clusterscoffee_chicken_thai_kebab5',\n",
       " 'ag_clusterscoffee_chicken_thai_kebab6',\n",
       " 'cluster_colors']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ward_variables.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cluster_var = [['med_income_2012_13','coffee'],['med_income_2012_13', 'chicken'], ['coffee','chicken'],\n",
    "              ['med_income_2012_13','coffee','chicken'], ['med_income_2012_13','coffee','chicken', 'thai', 'kebab'], \n",
    "              ['coffee','chicken', 'thai', 'kebab']]\n",
    "\n",
    "cluster_names=['income_coffee', 'income_chicken', 'coffee_chicken', 'income_coffee_chicken', 'income_coffee_chicken_thai_kebab',\n",
    "              'coffee_chicken_thai_kebab']\n",
    "\n",
    "for i in range(len(cluster_var)):\n",
    "\n",
    "    for x in range(7):\n",
    "\n",
    "        from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "        subset = ward_variables_subset[cluster_var[i]]\n",
    "\n",
    "        import sklearn as sk\n",
    "        scaled = sk.preprocessing.scale(subset)\n",
    "        AgClustering = AgglomerativeClustering(n_clusters=x+1)\n",
    "        AgClustering.fit(scaled)\n",
    "        AgClustering_labels = AgClustering.labels_\n",
    "        ward_variables['ag_clusters'+ cluster_names[i] + str(x)] = AgClustering_labels\n",
    "\n",
    "        #cloropleth\n",
    "        plt.clf()\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111, axisbg='w', frame_on=False)\n",
    "        plt.setp(ax.get_xticklabels(), fontsize=8)\n",
    "        plt.setp(ax.get_yticklabels(), fontsize=8)\n",
    "\n",
    "        # use a blue colour ramp - we'll be converting it to a map using cmap()\n",
    "        cmap = plt.get_cmap('gist_ncar')\n",
    "        pc = PatchCollection(ward_patches, alpha=1, lw = 0.1, edgecolor = '0')\n",
    "\n",
    "        # impose our colour map onto the patch collection\n",
    "        norm = Normalize()\n",
    "        pc.set_facecolor(cmap(norm(ward_variables['ag_clusters' + cluster_names[i] + str(x)].values)))\n",
    "        ax.add_collection(pc)\n",
    "\n",
    "        ax.set_xlim([xmin, xmax])\n",
    "        ax.set_ylim([ymin, ymax])\n",
    "        plt.title('Heirachical Clustering for Variables: ' + cluster_names[i] + str(x), weight = 'heavy', y=1.05)\n",
    "        ax.arrow(560000, 158000, 0, 4000, head_width=700, head_length=700, fc='k', ec='k')\n",
    "\n",
    "        fig.savefig('spatial_analysis/figures/cluster/' + cluster_names[i] + str(x) + '_cluster.png', dpi=200, figsize = (12,8),\n",
    "                   transparent = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      1\n",
       "1      2\n",
       "2      3\n",
       "3      0\n",
       "4      2\n",
       "5      2\n",
       "6      0\n",
       "7      3\n",
       "8      2\n",
       "9      3\n",
       "10     2\n",
       "11     1\n",
       "12     0\n",
       "13     2\n",
       "14     2\n",
       "15     1\n",
       "16     0\n",
       "17     3\n",
       "18     3\n",
       "19     0\n",
       "20     3\n",
       "21     1\n",
       "22     0\n",
       "23     2\n",
       "24     2\n",
       "25     0\n",
       "26     0\n",
       "27     0\n",
       "28     0\n",
       "29     0\n",
       "      ..\n",
       "595    0\n",
       "596    0\n",
       "597    0\n",
       "598    0\n",
       "599    0\n",
       "600    0\n",
       "601    0\n",
       "602    0\n",
       "603    0\n",
       "604    0\n",
       "605    0\n",
       "606    0\n",
       "607    0\n",
       "608    0\n",
       "609    0\n",
       "610    0\n",
       "611    0\n",
       "612    0\n",
       "613    0\n",
       "614    0\n",
       "615    0\n",
       "616    0\n",
       "617    0\n",
       "618    0\n",
       "619    0\n",
       "620    0\n",
       "621    0\n",
       "622    0\n",
       "623    0\n",
       "624    1\n",
       "Name: ag_clustersincome_coffee_chicken_thai_kebab3, dtype: int64"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ward_variables['ag_clustersincome_coffee_chicken_thai_kebab3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      #ff5050\n",
      "1      #66ff66\n",
      "2      #ffff66\n",
      "3      #33ccff\n",
      "4      #66ff66\n",
      "5      #66ff66\n",
      "6      #33ccff\n",
      "7      #ffff66\n",
      "8      #66ff66\n",
      "9      #ffff66\n",
      "10     #66ff66\n",
      "11     #ff5050\n",
      "12     #33ccff\n",
      "13     #66ff66\n",
      "14     #66ff66\n",
      "15     #ff5050\n",
      "16     #33ccff\n",
      "17     #ffff66\n",
      "18     #ffff66\n",
      "19     #33ccff\n",
      "20     #ffff66\n",
      "21     #ff5050\n",
      "22     #33ccff\n",
      "23     #66ff66\n",
      "24     #66ff66\n",
      "25     #33ccff\n",
      "26     #33ccff\n",
      "27     #33ccff\n",
      "28     #33ccff\n",
      "29     #33ccff\n",
      "        ...   \n",
      "595    #33ccff\n",
      "596    #33ccff\n",
      "597    #33ccff\n",
      "598    #33ccff\n",
      "599    #33ccff\n",
      "600    #33ccff\n",
      "601    #33ccff\n",
      "602    #33ccff\n",
      "603    #33ccff\n",
      "604    #33ccff\n",
      "605    #33ccff\n",
      "606    #33ccff\n",
      "607    #33ccff\n",
      "608    #33ccff\n",
      "609    #33ccff\n",
      "610    #33ccff\n",
      "611    #33ccff\n",
      "612    #33ccff\n",
      "613    #33ccff\n",
      "614    #33ccff\n",
      "615    #33ccff\n",
      "616    #33ccff\n",
      "617    #33ccff\n",
      "618    #33ccff\n",
      "619    #33ccff\n",
      "620    #33ccff\n",
      "621    #33ccff\n",
      "622    #33ccff\n",
      "623    #33ccff\n",
      "624    #ff5050\n",
      "Name: cluster_colors, dtype: object\n"
     ]
    }
   ],
   "source": [
    "ward_variables['ag_clustersincome_coffee_chicken_thai_kebab3']\n",
    "\n",
    "def cluster_colour (row, var):\n",
    "    if row[var] == 0:\n",
    "          return '#33ccff' #light blue\n",
    "    if row[var] == 1:\n",
    "          return '#ff5050' #orangy red\n",
    "    if row[var] == 2:\n",
    "          return '#66ff66' #green\n",
    "    if row[var] == 3:\n",
    "          return '#ffff66' #yellow\n",
    "\n",
    "#Apply function to create new dataframe coloumn\n",
    "ward_variables['cluster_colors'] = ward_variables.apply(lambda row: cluster_colour(row,'ag_clustersincome_coffee_chicken_thai_kebab3'),\n",
    "                                                        axis=1)\n",
    "print ward_variables['cluster_colors']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10 - Export .csvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ward_variables.to_csv('Data/spatial_analysis_wards_output.csv')\n",
    "global_stats_df.to_csv('Data/spatial_analysis_global_output.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Step 11 - Make charts for website content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Scatter Plots\n",
    "\n",
    "for i in range(len(tokens_lst)):\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    y = ward_variables['med_income_2012_13']\n",
    "    x = ward_variables[tokens_lst[i] + '_smooth_lq']\n",
    "\n",
    "    plt.setp(ax.get_xticklabels(), fontsize=8)\n",
    "    plt.setp(ax.get_yticklabels(), fontsize=8)\n",
    "\n",
    "    scatter = plt.scatter(x, y, marker='^', color = tokens_color[i]) \n",
    "    m, b = np.polyfit(x, y, deg=1)\n",
    "    plt.plot(x, m*x + b, '-', c='red')\n",
    "    ax.patch.set_alpha(0)\n",
    "\n",
    "    plt.title(\"Density of Token '\" + tokens_lst[i] + \"' by Income per Ward\", weight = 'normal', y=1.05)\n",
    "    plt.xlabel(\"Location Quotient\")\n",
    "    plt.ylabel('Income ($)')\n",
    "\n",
    "\n",
    "    plt.xlim(xmin = 0)\n",
    "    plt.ylim(20000,90000)\n",
    "    \n",
    "    fig.savefig('spatial_analysis/figures/scatter_plot/' + tokens_lst[i] + '_income_scatter.png', dpi=200, figsize = (12,8),\n",
    "                   transparent = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Histograms\n",
    "\n",
    "\n",
    "for i in range (len(tokens_lst)):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    x = ward_variables[tokens_lst[i]  + '_smooth_lq']\n",
    "\n",
    "    plt.setp(ax.get_xticklabels(), fontsize=8)\n",
    "    plt.setp(ax.get_yticklabels(), fontsize=8)\n",
    "    n, bins, patches = plt.hist(x, 50, normed=1, facecolor=tokens_color[i], alpha=0.75) \n",
    "    ax.patch.set_alpha(0)\n",
    "\n",
    "    plt.title(\"Histogram of Density of Token '\" + tokens_lst[i]  + \"' per Ward\", weight = 'normal', y=1.05)\n",
    "    plt.xlabel(\"Location Quotient\")\n",
    "    plt.ylabel('Probability')\n",
    "\n",
    "\n",
    "    #plt.show()\n",
    "\n",
    "    fig.savefig('spatial_analysis/figures/histogram/' + tokens_lst[i] + '_histogram.png', dpi=200, figsize = (12,8),\n",
    "                       transparent = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
