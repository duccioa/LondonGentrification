{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatial Analysis of Food Business Data\n",
    "\n",
    "### Exploring gentrification through consumer preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#LOAD REQUIRED PACKAGES\n",
    "\n",
    "#Stats and data structures\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "#Matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib.collections import PatchCollection\n",
    "from descartes import PolygonPatch\n",
    "\n",
    "#Coordinate system transformation\n",
    "import pyproj\n",
    "from pyproj import Proj, transform\n",
    "\n",
    "#Shapefile reading and manipulataion\n",
    "import shapefile\n",
    "import shapely\n",
    "from shapely.geometry import Polygon\n",
    "from shapely.geometry import Point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Data import and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Set working directory\n",
    "import os\n",
    "os.chdir(\"C:/Users/Claire/Google Drive/LondonGentrification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import .csv data \n",
    "\n",
    "#Word tokens \n",
    "tokens_df = pd.read_csv(\"Data/FoodPremises/tokens_spatial.csv\")\n",
    "\n",
    "#Food businesses\n",
    "food_bus_df = pd.read_csv(\"data/FoodPremises/london_premises.csv\")\n",
    "#Remove records with no coordinates\n",
    "food_bus_df = food_bus_df.loc[food_bus_df['Latitude']>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BusinessID                float64\n",
      "BusinessName               object\n",
      "BusinessType               object\n",
      "BusinessTypeID            float64\n",
      "ConfidenceInManagement    float64\n",
      "Hygiene                   float64\n",
      "LocalAuthorityCode        float64\n",
      "LocalAuthorityName         object\n",
      "PostCode                   object\n",
      "RatingValue               float64\n",
      "Structural                float64\n",
      "Token                      object\n",
      "lat                       float64\n",
      "lon                       float64\n",
      "dtype: object\n",
      "Index                       int64\n",
      "BusinessName               object\n",
      "BusinessType               object\n",
      "BusinessTypeID              int64\n",
      "PostCode                   object\n",
      "RatingValue               float64\n",
      "RatingDate                 object\n",
      "LocalAuthorityCode          int64\n",
      "LocalAuthorityName         object\n",
      "Hygiene                   float64\n",
      "Structural                float64\n",
      "ConfidenceInManagement    float64\n",
      "Longitude                 float64\n",
      "Latitude                  float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "#Check column names and data types\n",
    "print tokens_df.dtypes\n",
    "print food_bus_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to add eastings and northings from lat an lon\n",
    "#Add eastings and northings to word tokens\n",
    "\n",
    "def transform_coordinates (data,input_espg, output_espg, input_x, input_y):\n",
    "\n",
    "    #define input and output projection\n",
    "    input_projection = pyproj.Proj(\"+init=\" + input_espg) #wgs84\n",
    "    output_projection = pyproj.Proj(\"+init=\" + output_espg) #osgb36\n",
    "\n",
    "    eastings = []\n",
    "    northings = []\n",
    "\n",
    "    x_list = input_x.tolist()\n",
    "    y_list = input_y.tolist()\n",
    "\n",
    "    for i in range (len(input_x)):\n",
    "        x = x_list[i]\n",
    "        y = y_list[i]\n",
    "        new_x, new_y = pyproj.transform(input_projection, output_projection, x, y)\n",
    "        eastings.append(new_x)\n",
    "        northings.append(new_y)\n",
    "\n",
    "    #Add to tokens dataframe\n",
    "    data['eastings'] = eastings\n",
    "    data['northings'] = northings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Apply funtion to dataframe - convert tokens and businesses to eastings and northings\n",
    "\n",
    "transform_coordinates(tokens_df, \"EPSG:4326\", \"EPSG:27700\", \n",
    "                       tokens_df['lon'], tokens_df['lat'])\n",
    "\n",
    "transform_coordinates(food_bus_df, \"EPSG:4326\", \"EPSG:27700\", \n",
    "                       food_bus_df['Longitude'], food_bus_df['Latitude'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create subsetted dataframes for each token of interest for operations later\n",
    "\n",
    "#List of tokens of interest to perform iteration operations:\n",
    "tokens_lst = ['cafe', 'coffee', 'pizza', 'wine', 'sushi', 'thai', 'chicken', \n",
    "                   'fried', 'fish', 'kebab', 'costcutter', 'waitrose', 'sainsburys', 'tesco', 'grill']\n",
    "\n",
    "#Colour codes to associate with each token\n",
    "tokens_color = ['#9fc54d', '#75c156', '#33a457', '#71b67b', '#61bdf0', '#1e71b8', '#e03c00', \n",
    "                '#e56000', '#f08c00', '#ffea00', '#18563e', '#729f1e', '#ee7a01', '#0053a0', '#1ea86c']\n",
    "\n",
    "df = {}\n",
    "\n",
    "for x in tokens_lst:\n",
    "    df[x] = pd.DataFrame(tokens_df.loc[tokens_df['Token'] == x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Import london wards shapefile and save as matplot lib patches for plotting\n",
    "\n",
    "#Load the shapefile of polygons and convert it to shapely polygon objects\n",
    "polygons_sf = shapefile.Reader(\"Data/ESRI/london_wards.shp\")\n",
    "polygons_south_sf = shapefile.Reader(\"Data/ESRI/south_london_wards.shp\") #Separate shapefile for south london wards\n",
    "polygons_north_sf = shapefile.Reader(\"Data/ESRI/north_london_wards.shp\") #Separate shapefile for north london wards\n",
    "\n",
    "polygon_shapes = polygons_sf.shapes() #Create object with shapes\n",
    "polygon_points = [q.points for q in polygon_shapes ] #Extract point information from shapes\n",
    "polygons = [Polygon(q) for q in polygon_points]\n",
    "\n",
    "#Create matplotlib patches from shapely polygons for mapping figures\n",
    "ward_patches = []\n",
    "for x in range (len(polygons)):\n",
    "    a = PolygonPatch(polygons[x])\n",
    "    ward_patches.append(a)\n",
    "    \n",
    "#Define bounding box of the shapefile (eastings and northings) - plus boundary increase for figures\n",
    "xmin = polygons_sf.bbox[0] - 2000\n",
    "xmax = polygons_sf.bbox[2] + 2000\n",
    "ymin = polygons_sf.bbox[1] - 2000\n",
    "ymax = polygons_sf.bbox[3] + 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create wards dataframe from shapefile\n",
    "\n",
    "#polygons_sf.fields #Access shapefile fields\n",
    "#polygons_sf.records #Access shapefile records\n",
    "\n",
    "records = polygons_sf.records()\n",
    "\n",
    "position = []\n",
    "ward_name = []\n",
    "ward_code = []\n",
    "\n",
    "for x in range (len(records)):\n",
    "    position.append(records[x][0] - 1)\n",
    "    ward_name.append(records[x][1])\n",
    "    ward_code.append(records[x][2])\n",
    "\n",
    "#Compile dataframe\n",
    "ward_variables = pd.DataFrame({'position': position, 'ward_name': ward_name,'ward_code':ward_code})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create field to define wards north and south of the river\n",
    "\n",
    "records_north = polygons_north_sf.records()\n",
    "records_south = polygons_south_sf.records()\n",
    "\n",
    "code_north = []\n",
    "n=[]\n",
    "code_south = []\n",
    "s=[]\n",
    "\n",
    "for x in range (len(records_north)):\n",
    "    code_north.append(records_north[x][2])\n",
    "    n.append('n')\n",
    "    \n",
    "for x in range (len(records_south)):\n",
    "    code_south.append(records_south[x][2])\n",
    "    s.append('s')\n",
    "\n",
    "#Compile dataframe\n",
    "north_wards_df = pd.DataFrame({'code':code_north, 'n_s':n})\n",
    "south_wards_df = pd.DataFrame({'code':code_south, 'n_s':s})\n",
    "north_south_df = north_wards_df.append(south_wards_df)\n",
    "\n",
    "#Merge north south dataframe to ward variables dataframe by ward code\n",
    "ward_variables = pd.merge(ward_variables, north_south_df,left_on='ward_code', right_on='code' , how='inner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Calculate centroids for each ward polygon and add to ward variables dataframe\n",
    "centroids = []\n",
    "\n",
    "for x in range(len(polygons)):\n",
    "    a = polygons[x].centroid\n",
    "    centroids.append(a)\n",
    "    \n",
    "ward_variables['centroid'] = centroids\n",
    "\n",
    "centroid_x = []\n",
    "centroid_y = []\n",
    "\n",
    "\n",
    "for x in ward_variables['centroid']:\n",
    "    centroid_x.append(x.x)\n",
    "    centroid_y.append(x.y)\n",
    "    \n",
    "ward_variables['centroid_x'] = centroid_x \n",
    "ward_variables['centroid_y'] = centroid_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Join income data to ward variables dataframe\n",
    "\n",
    "#Import income .csv\n",
    "ward_income = pd.read_csv(\"Data/modelled-household-income-estimates-wards.csv\")\n",
    "\n",
    "#Change code for city of london to be consistent with shapefile\n",
    "ward_income.set_value(0,['Code'], 'E05001554')\n",
    "\n",
    "#Join to ward variables dataframe\n",
    "ward_variables = ward_variables.merge(ward_income[['Code','Median 2012_13']], left_on='ward_code', right_on = 'Code')\n",
    "\n",
    "#Rename Column\n",
    "ward_variables=ward_variables.rename(columns = {'Median 2012_13':'med_income_2012_13'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Categorise as high or low income (below or above median)\n",
    "\n",
    "#Caluculate median\n",
    "med_income = np.median(ward_variables['med_income_2012_13'])\n",
    "\n",
    "#Define function for categories\n",
    "def income_category (row):\n",
    "    if row['med_income_2012_13'] >= med_income:\n",
    "          return 'high'\n",
    "    if row['med_income_2012_13'] < med_income:\n",
    "          return 'low'\n",
    "\n",
    "#Apply function to create new dataframe coloumn\n",
    "ward_variables['income_category'] = ward_variables.apply(lambda row: income_category (row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import occupation data by ward\n",
    "\n",
    "#Import occupation .csv\n",
    "ward_occupation = pd.read_csv(\"Data/ward_occupation_data.csv\")\n",
    "\n",
    "#Calculate a new field to show the percentage of the employed population employed as professionals\n",
    "ward_occupation['combined_professionals_pct'] = ((ward_occupation['Sex: All persons; Occupation: 1. Managers, directors and senior officials; measures: Value'] +\n",
    "    ward_occupation['Sex: All persons; Occupation: 2. Professional occupations; measures: Value'])/\n",
    "    ward_occupation['Sex: All persons; Occupation: All categories: Occupation; measures: Value']) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Merge ward occupation information to be stored in the ward variables dataframe\n",
    "ward_variables = ward_variables.merge(ward_occupation[['geography code', 'combined_professionals_pct']], left_on='ward_code', right_on='geography code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Add colours for web mapping - define function\n",
    "\n",
    "#FUNCTION TO ASSIGN COLOURS (EQUAL INTERVAL)\n",
    "import numpy as np\n",
    "\n",
    "def color_assign (row, data, var, N, c):\n",
    "    \"\"\"\n",
    "    :param row: marker for apply function of dataframe\n",
    "    :param data: column of dataframe for calculations\n",
    "    :param var: column name\n",
    "    :param N: number of desired colour intervals\n",
    "    :param x: desired colour as tring either 'r' (red), 'g' (green), 'p'(purple) or 'b' (blue)\n",
    "    :return: <returns a colour accordining to the location of the value >returns the colour according to the interval the value belongs to\n",
    "    :example of application to a Selected column of a dataframe:\n",
    "    df['Color'] = df.apply(lambda row: color_assign (row, df['<Selected Column>'], '<Selected Column>'), axis=1)\n",
    "    \"\"\"\n",
    "    #CREATE A SPECTRUM OF COLOURS\n",
    "    increment = 255/(N+1)\n",
    "    RGB_tuples = []\n",
    "    \n",
    "    for x in range(N):\n",
    "\n",
    "        if c == 'r':\n",
    "            R = 255\n",
    "            G = (230-(x*increment))\n",
    "            B = (230-(x*increment))\n",
    "\n",
    "        if c == 'g':        \n",
    "            R = (230-(x*increment))\n",
    "            G = (255-(x/2*increment))\n",
    "            B = (230-(x*increment))\n",
    "\n",
    "        if c == 'p':        \n",
    "            R = (230-(x/4*increment))\n",
    "            G = (230-(x*increment))\n",
    "            B = 255\n",
    "            \n",
    "        if c == 'b':        \n",
    "            R = (230-(x*increment))\n",
    "            G = (230-(x*increment))\n",
    "            B = 255\n",
    "            \n",
    "        RGB_tuples.append((R,G,B))\n",
    "\n",
    "    #Convert to hex color format for mapping\n",
    "    hex_colour = []\n",
    "    for x in RGB_tuples:\n",
    "        hexa = '#%02x%02x%02x' % (x)\n",
    "        hex_colour.append(hexa)\n",
    "\n",
    "    minimum = data.min()\n",
    "    maximum = data.max()\n",
    "\n",
    "    N=len(hex_colour)\n",
    "    intervals = np.linspace(minimum, maximum, num=N+1)\n",
    "    for x in range(N):\n",
    "        if (row[var] <= intervals[x+1]) & (row[var] >= intervals [x]):\n",
    "            return hex_colour[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Assign color range for income\n",
    "ward_variables['income_color'] = ward_variables.apply(lambda row: color_assign(row, \n",
    "                                                ward_variables['med_income_2012_13'], 'med_income_2012_13', 7, 'b'), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Save intervals to file for website legend\n",
    "\n",
    "minimum = ward_variables['med_income_2012_13'].min()\n",
    "maximum = ward_variables['med_income_2012_13'].max()\n",
    "income_intervals_df = pd.DataFrame()\n",
    "intervals = np.linspace(minimum, maximum, num=8)\n",
    "income_intervals_df['income_intervals'] = intervals[1:8]\n",
    "income_intervals_df.to_csv('Data/income_intervals_legend.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Calculate number of each token within each ward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert token coordinates to shapely point file for each dataframe\n",
    "from shapely.geometry import Point\n",
    "\n",
    "token_points = {}\n",
    "token_points_coords = {}\n",
    "\n",
    "for x in tokens_lst:\n",
    "    #Make shapely points\n",
    "    token_points[x] = [Point(xy) for xy in zip(df[x]['eastings'], df[x]['northings'])]\n",
    "    \n",
    "    #convert shapely points into coordinate tuples\n",
    "    point_coords = []\n",
    "    for i in range(len(token_points[x])):\n",
    "        a = ([token_points[x][i].x, token_points[x][i].y])\n",
    "        point_coords.append(a)\n",
    "    token_points_coords[x] = point_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Build a spatial index based on the bounding boxes of the polygons\n",
    "from rtree import index\n",
    "idx = index.Index()\n",
    "count = -1\n",
    "for q in polygon_shapes:\n",
    "    count +=1\n",
    "    idx.insert(count, q.bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Assign one or more matching polygons to each point\n",
    "\n",
    "for x in tokens_lst:\n",
    "    matches = []\n",
    "\n",
    "    for i in range(len(df[x]['Token'])): #Iterate through each point\n",
    "        temp= None\n",
    "        #print \"Point \", i\n",
    "\n",
    "        #Iterate only through the bounding boxes which contain the point\n",
    "        for j in idx.intersection(token_points_coords[x][i]):\n",
    "            #Verify that point is within the polygon itself not just the bounding box\n",
    "            if token_points[x][i].within(polygons[j]):\n",
    "                temp=j\n",
    "                break\n",
    "        matches.append(temp) #Either the first match found, or None for no matches\n",
    "    \n",
    "    df[x]['ward_no'] = matches\n",
    "    \n",
    "    #Merge ward name and code information based on the polygons matched in the spatial join operation above\n",
    "    df[x] = df[x].merge(ward_variables[['position','ward_name', 'ward_code','n_s']], left_on='ward_no', right_on='position')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Names of wards\n",
    "london_wards = ward_variables['ward_name']\n",
    "\n",
    "#Count the number of tokens for each ward and join to ward_variables dataframe\n",
    "for x in tokens_lst:\n",
    "    token_count = []\n",
    "    for b in london_wards:\n",
    "            temp_df = df[x].loc[(df[x]['Token'] == x) & \n",
    "                                        (df[x]['ward_name'] == b)]\n",
    "            token_count.append(len(temp_df.index))\n",
    "    ward_variables[x + '_count'] = token_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Create KDE surface and attach KDE values to ward centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate surfances are used for wards north and south of the river to take into account the accessibility barrier effect of the thames. This ensures that a high concerntration of tokens along one side of the river does not effect calculations for the other side of the river."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#In order to record density surface for each token\n",
    "kernels = {}\n",
    "kernels_n = {}\n",
    "kernels_s = {}\n",
    "\n",
    "for x in tokens_lst:\n",
    "    \n",
    "    #Set up north south subsets\n",
    "    n_subset_df = df[x].loc[df[x]['n_s']=='n']\n",
    "    s_subset_df = df[x].loc[df[x]['n_s']=='s']\n",
    "\n",
    "    #Set up grid and KDE calculation\n",
    "    X, Y = np.mgrid[xmin:xmax:100j, ymin:ymax:100j]\n",
    "    positions = np.vstack([X.ravel(), Y.ravel()])\n",
    "    \n",
    "    #Calculate entiire density surface\n",
    "    values_coffee = np.vstack([df[x]['eastings'], df[x]['northings']])\n",
    "    kernels[x] = stats.gaussian_kde(values_coffee, 0.1) #Bandwidth set ot 0.1\n",
    "    Z = np.reshape(kernels[x](positions).T, X.shape)\n",
    "    \n",
    "    #Calculate north density surface\n",
    "    values_coffee_n = np.vstack([n_subset_df['eastings'], n_subset_df['northings']])\n",
    "    kernels_n[x] = stats.gaussian_kde(values_coffee_n, 0.1) #Bandwidth set ot 0.1\n",
    "\n",
    "    #Calculate south density surface\n",
    "    values_coffee_s = np.vstack([s_subset_df['eastings'], s_subset_df['northings']])\n",
    "    kernels_s[x] = stats.gaussian_kde(values_coffee_s, 0.1) #Bandwidth set ot 0.1\n",
    "    \n",
    "    #Plot KDE surface and save\n",
    "    token = x\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    fig = plt.figure(figsize=(12,8))\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.setp(ax.get_xticklabels(), fontsize=8)\n",
    "    plt.setp(ax.get_yticklabels(), fontsize=8)\n",
    "    \n",
    "    #Add london wards as patches\n",
    "    ax.add_collection(PatchCollection(ward_patches, alpha=1, facecolor='None', lw = 0.1, \n",
    "                                      edgecolor = '0'))\n",
    "    kde_surface = ax.imshow(np.rot90(Z), cmap=plt.cm.gist_earth_r,\n",
    "               extent=[xmin, xmax, ymin, ymax])\n",
    "    ax.plot(df[x]['eastings'], df[x]['northings'], 'k.', markersize=2)\n",
    "    ax.set_xlim([xmin, xmax])\n",
    "    ax.set_ylim([ymin, ymax])\n",
    "    plt.title(\"Kernel Density Surface (KDE) for Word Token '\" + token +\"'\", weight = 'heavy', y=1.05)\n",
    "    ax.arrow(560000, 158000, 0, 4000, head_width=700, head_length=700, fc='k', ec='k')\n",
    "\n",
    "    #ax.set_ylim([ymin, ymax])\n",
    "    plt.colorbar(kde_surface, cmap=plt.cm.gist_earth_r )\n",
    "\n",
    "    #plt.show()\n",
    "    fig.savefig('spatial_analysis/figures/KDE/' + token + '_kde.png', dpi=200, figsize = (12,8), transparent=True)\n",
    "\n",
    "    #Evaluate density at each ward centroid\n",
    "    \n",
    "    ward_variables[x + '_kde'] = \"\"\n",
    "    \n",
    "    for y in range(len(ward_variables['ward_name'])):\n",
    "        \n",
    "        #Use north density surface if ward is located north of the river\n",
    "        if ward_variables['n_s'].iloc[y] == 'n':\n",
    "            kde = kernels_n[x].evaluate([ward_variables['centroid'].iloc[y].x, \n",
    "                                 ward_variables['centroid'].iloc[y].y])\n",
    "            ward_variables.set_value(y, [x + '_kde'], kde)\n",
    "        \n",
    "        #Use south density surface if ward is located south of the river\n",
    "        if ward_variables['n_s'].iloc[y] == 's':\n",
    "            kde = kernels_s[x].evaluate([ward_variables['centroid'].iloc[y].x, \n",
    "                                     ward_variables['centroid'].iloc[y].y])\n",
    "            ward_variables.set_value(y, [x + '_kde'], kde)\n",
    "    \n",
    "    #Ensure output is type 'float' for later operations\n",
    "    ward_variables[x + '_kde'] = ward_variables[x + '_kde'].astype(float)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Calculate number of businesses within each ward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert business location coordinates to shapely point file for each dataframe\n",
    "from shapely.geometry import Point\n",
    "\n",
    "#Make shapely points\n",
    "food_bus_points = [Point(xy) for xy in zip(food_bus_df['eastings'], food_bus_df['northings'])]\n",
    "    \n",
    "#convert shapely points into coordinate tuples\n",
    "point_coords = []\n",
    "for i in range(len(food_bus_points)):\n",
    "    a = ([food_bus_points[i].x, food_bus_points[i].y])\n",
    "    point_coords.append(a)\n",
    "food_bus_coords = point_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Perform spatial join operation (using same bounding box index above to reduce the time)\n",
    "\n",
    "matches = []\n",
    "\n",
    "for i in range(len(food_bus_df['Latitude'])): #Iterate through each point\n",
    "    temp= None\n",
    "    #print \"Point \", i\n",
    "\n",
    "    #Iterate only through the bounding boxes which contain the point\n",
    "    for j in idx.intersection(food_bus_coords[i]):\n",
    "        #Verify that point is within the polygon itself not just the bounding box\n",
    "        if food_bus_points[i].within(polygons[j]):\n",
    "            temp=j\n",
    "            break\n",
    "    matches.append(temp) #Either the first match found, or None for no matches\n",
    "\n",
    "#Join ward information to the food business datframe based on the matching ward shapes\n",
    "food_bus_df['ward_no'] = matches\n",
    "food_bus_df = food_bus_df.merge(ward_variables[['position','ward_name', 'ward_code', 'n_s']], left_on='ward_no', right_on='position')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Calculate number of businesses within each ward by calculating the length of dataframe slices for each ward\n",
    "\n",
    "token_count = []\n",
    "\n",
    "for b in london_wards:\n",
    "        temp_df = food_bus_df.loc[food_bus_df['ward_name'] == b]\n",
    "        token_count.append(len(temp_df.index))\n",
    "        \n",
    "ward_variables['all_business_count'] = token_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Step 5 - Calculate location quotient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The location quotient is a double ratio measure with the ratio between the count for the word token against the count for all food businesses as a ratio between the local and global ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Calculate LQ for each token\n",
    "#Word token number for each ward centroid divided by all business number for each ward centroid divided by global aggregate\n",
    "\n",
    "for x in tokens_lst:\n",
    "    ward_variables[x +'_lq'] = (ward_variables[x + '_count']/(ward_variables['all_business_count']))/(len(df[x])/float(len(food_bus_df.index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Visualise the LQ\n",
    "\n",
    "for x in tokens_lst:\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    fig = plt.figure(figsize=(12,8))\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.setp(ax.get_xticklabels(), fontsize=8)\n",
    "    plt.setp(ax.get_yticklabels(), fontsize=8)\n",
    "\n",
    "    #Add london wards as patches\n",
    "    cmap = plt.get_cmap('Blues')\n",
    "    test = PatchCollection(ward_patches, alpha=1, facecolor=cmap(ward_variables[x + '_lq']), lw = 0.1, \n",
    "                                          edgecolor = '0')\n",
    "    ax.add_collection(test)\n",
    "    \n",
    "    #Add token locations as red points\n",
    "    ax.plot(df[x]['eastings'], df[x]['northings'], 'k.', markersize=4, markerfacecolor='red', alpha=0.8)\n",
    "    \n",
    "    #Titles and figure text settings\n",
    "    ax.set_xlim([xmin, xmax])\n",
    "    ax.set_ylim([ymin, ymax])\n",
    "    plt.title(\"Location Quotient (LQ) for Number of Word Token '\" + token +\"' by Ward\", weight = 'heavy', y=1.05)\n",
    "    ax.arrow(560000, 158000, 0, 4000, head_width=700, head_length=700, fc='k', ec='k')\n",
    "\n",
    "    #Add colourbar\n",
    "    m = cm.ScalarMappable(cmap=cm.Blues)\n",
    "    m.set_array(ward_variables[x + '_lq'])\n",
    "    plt.colorbar(m)\n",
    "    \n",
    "    #Save figures to new folder\n",
    "    fig.savefig('spatial_analysis/figures/LQ_token_no/' + x + '_LQ_no.png', dpi=200, figsize = (12,8), transparent=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 - Calculate smoothed location quotient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The smoothed location quotient has been calculated using the kernel density estimate evaluations performed above for each ward centroid rather than a count of businesses in each ward. This measure smooths out the arbitrary effect of ward boundaries. \n",
    "\n",
    "The location quotient is a double ratio measure with the ratio between the KDE for the word token against the KDE for all food businesses as a ratio between the local and global ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Calculate the ward centroid KDE for all businesses in order to complete operation\n",
    "\n",
    "#Calculate kde for all wards\n",
    "n_subset_df = food_bus_df.loc[food_bus_df['n_s']=='n']\n",
    "s_subset_df = food_bus_df.loc[food_bus_df['n_s']=='s']\n",
    "\n",
    "#Add values for LQ for all businesses by ward\n",
    "#Set up grid and KDE calculation\n",
    "X, Y = np.mgrid[xmin:xmax:100j, ymin:ymax:100j]\n",
    "positions = np.vstack([X.ravel(), Y.ravel()])\n",
    "values = np.vstack([food_bus_df['eastings'], food_bus_df['northings']])\n",
    "kernels = stats.gaussian_kde(values, 0.1) #Bandwidth set ot 0.1\n",
    "Z = np.reshape(kernels(positions).T, X.shape)\n",
    "\n",
    "#Calculate north density surface\n",
    "values_coffee_n = np.vstack([n_subset_df['eastings'], n_subset_df['northings']])\n",
    "kernels_n = stats.gaussian_kde(values_coffee_n, 0.1) #Bandwidth set ot 0.1\n",
    "\n",
    "#Calculate north density surface\n",
    "values_coffee_s = np.vstack([s_subset_df['eastings'], s_subset_df['northings']])\n",
    "kernels_s = stats.gaussian_kde(values_coffee_s, 0.1) #Bandwidth set ot 0.1\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "ax = fig.add_subplot(111)\n",
    "plt.setp(ax.get_xticklabels(), fontsize=8)\n",
    "plt.setp(ax.get_yticklabels(), fontsize=8)\n",
    "\n",
    "#Add london wards as patches\n",
    "ax.add_collection(PatchCollection(ward_patches, alpha=1, facecolor='None', lw = 0.1, \n",
    "                                      edgecolor = '0'))\n",
    "kde_surface = ax.imshow(np.rot90(Z), cmap=plt.cm.gist_earth_r,\n",
    "               extent=[xmin, xmax, ymin, ymax])\n",
    "ax.plot(food_bus_df['eastings'], food_bus_df['northings'], 'k.', markersize=2)\n",
    "ax.set_xlim([xmin, xmax])\n",
    "ax.set_ylim([ymin, ymax])\n",
    "plt.title(\"Kernel Density Surface (KDE) for All Food Businesses\", weight = 'heavy', y=1.05)\n",
    "ax.arrow(560000, 158000, 0, 4000, head_width=700, head_length=700, fc='k', ec='k')\n",
    "\n",
    "#ax.set_ylim([ymin, ymax])\n",
    "plt.colorbar(kde_surface, cmap=plt.cm.gist_earth_r )\n",
    "\n",
    "#plt.show()\n",
    "fig.savefig('spatial_analysis/figures/KDE/allfoodbusiness_kde.png', dpi=200, figsize = (12,8), transparent=True)\n",
    "\n",
    "#Evaluate density at each ward centroid\n",
    "ward_kde = []\n",
    "    \n",
    "for y in range(len(ward_variables['ward_name'])):\n",
    "    kde = kernels.evaluate([ward_variables['centroid'].iloc[y].x, \n",
    "                             ward_variables['centroid'].iloc[y].y])\n",
    "    ward_kde.append(kde[0])\n",
    "        \n",
    "ward_variables['all_business_kde'] = \"\"\n",
    "\n",
    "for y in range(len(ward_variables['ward_name'])):\n",
    "     #Use north density surface if ward is located north of the river\n",
    "    if ward_variables['n_s'].iloc[y] == 'n':\n",
    "        kde = kernels_n.evaluate([ward_variables['centroid'].iloc[y].x, \n",
    "                                 ward_variables['centroid'].iloc[y].y])\n",
    "        ward_variables.set_value(y, ['all_business_kde'], kde)\n",
    "        \n",
    "    #Use south density surface if ward is located south of the river\n",
    "    if ward_variables['n_s'].iloc[y] == 's':\n",
    "        kde = kernels_s.evaluate([ward_variables['centroid'].iloc[y].x, \n",
    "                                     ward_variables['centroid'].iloc[y].y])\n",
    "        ward_variables.set_value(y, ['all_business_kde'], kde)\n",
    "\n",
    "ward_variables['all_business_kde'] = ward_variables['all_business_kde'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Calculate the smooth location quotient for each ward based on centroid KDEs\n",
    "for x in tokens_lst:\n",
    "    ward_variables[x +'_smooth_lq'] = (ward_variables[x + '_kde']/(ward_variables['all_business_kde']))/(sum(ward_variables[x + '_kde'])/float(sum(ward_variables['all_business_kde'])))\n",
    "    #Convert to float\n",
    "    ward_variables[x +'_smooth_lq'] = ward_variables[x +'_smooth_lq'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Colour set for visulisation of location quotient (quantiles)\n",
    "\n",
    "color_list = ['#0066ff', '#00FFFF', '#ffff3C', '#ff00ff', '#cc0099']\n",
    "interval_list = ['Max Interval 1', 'Max Interval 2','Max Interval 3','Max Interval 4','Max Interval 5']\n",
    "\n",
    "#Define function for categories\n",
    "def lq_color (row, df, var):\n",
    "\n",
    "    if row[var] <= np.percentile(df[var], 20):\n",
    "          return color_list[0]\n",
    "    elif row[var] <= np.percentile(df[var], 40):\n",
    "          return color_list[1]\n",
    "    elif row[var] <= np.percentile(df[var], 60):\n",
    "          return color_list[2]\n",
    "    elif row[var] <= np.percentile(df[var], 80):\n",
    "          return color_list[3]\n",
    "    elif row[var] <= np.percentile(df[var], 100):\n",
    "          return color_list[4] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Calculate colour fields (bright colours) for location quotients using function defined above\n",
    "for x in tokens_lst:\n",
    "    ward_variables[x +'_smooth_lq_color'] = ward_variables.apply(lambda row: lq_color(row, \n",
    "                                                    ward_variables, x +'_smooth_lq'), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Generate dataframe with legend for quantiles\n",
    "\n",
    "legend_intervals_df = pd.DataFrame(index=interval_list)\n",
    "\n",
    "for x in tokens_lst:\n",
    "    intervals = [np.percentile(ward_variables[x +'_smooth_lq'], 20), \n",
    "                 np.percentile(ward_variables[x +'_smooth_lq'] , 40), \n",
    "                 np.percentile(ward_variables[x +'_smooth_lq'], 60),\n",
    "                 np.percentile(ward_variables[x +'_smooth_lq'] , 80), \n",
    "                 np.percentile(ward_variables[x +'_smooth_lq'] , 100)]\n",
    "    legend_intervals_df [x +'_smooth_lq'] = intervals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "legend_intervals_df.to_csv('Data/legend_location_quotient_intervals.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Visualise the Smooth LQ\n",
    "\n",
    "for x in tokens_lst:\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    fig = plt.figure(figsize=(12,8))\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.setp(ax.get_xticklabels(), fontsize=8)\n",
    "    plt.setp(ax.get_yticklabels(), fontsize=8)\n",
    "\n",
    "\n",
    "    #Add london wards as patches\n",
    "    cmap = plt.get_cmap('BuGn')\n",
    "    test = PatchCollection(ward_patches, alpha=1, facecolor=cmap(ward_variables[x + '_smooth_lq']), lw = 0.1, \n",
    "                                          edgecolor = '0')\n",
    "    ax.add_collection(test)\n",
    "    \n",
    "    #Plot points\n",
    "    ax.plot(df[x]['eastings'], df[x]['northings'], 'k.', markersize=4, markerfacecolor='red', alpha=0.8)\n",
    "    \n",
    "    #Axis options\n",
    "    ax.set_xlim([xmin, xmax])\n",
    "    ax.set_ylim([ymin, ymax])\n",
    "    plt.title(\"KDE Location Quotient (LQ) for Word Token '\" + x +\"'\", weight = 'heavy', y=1.05)\n",
    "    ax.arrow(560000, 158000, 0, 4000, head_width=700, head_length=700, fc='k', ec='k')\n",
    "\n",
    "    #Add Color bar\n",
    "    m = cm.ScalarMappable(cmap=cm.BuGn)\n",
    "    m.set_array(ward_variables[x + '_smooth_lq'])\n",
    "    plt.colorbar(m)\n",
    "    \n",
    "    #Save figure\n",
    "    fig.savefig('spatial_analysis/figures/LQ_KDE_smooth/' + x + '_smooth_LQ.png', dpi=200, figsize = (12,8), transparent = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 - Moran's I Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Global Moran's I\n",
    "\n",
    "import pysal\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#Create rooks weights matrix\n",
    "w = pysal.rook_from_shapefile(\"Data/ESRI/london_wards.shp\")\n",
    "\n",
    "morans_value = []\n",
    "morans_EI = []\n",
    "morans_p = []\n",
    "\n",
    "for x in tokens_lst:\n",
    "    #Import variable as array\n",
    "    y = np.array(ward_variables[x + '_kde'].astype(float))\n",
    "\n",
    "    #Calculate Moran's I\n",
    "    mi = pysal.Moran(y, w, two_tailed=False)\n",
    "\n",
    "    \n",
    "    morans_value.append(\"%.3f\"%mi.I)\n",
    "    morans_EI.append(mi.EI)\n",
    "    morans_p.append(\"%.5f\"%mi.p_norm)\n",
    "\n",
    "global_stats_df = pd.DataFrame({'morans_value': morans_value, 'morans_EI': morans_EI, 'morans_p':morans_p}, \n",
    "                                index=tokens_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>morans_EI</th>\n",
       "      <th>morans_p</th>\n",
       "      <th>morans_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cafe</th>\n",
       "      <td>-0.001603</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coffee</th>\n",
       "      <td>-0.001603</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pizza</th>\n",
       "      <td>-0.001603</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wine</th>\n",
       "      <td>-0.001603</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sushi</th>\n",
       "      <td>-0.001603</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thai</th>\n",
       "      <td>-0.001603</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chicken</th>\n",
       "      <td>-0.001603</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fried</th>\n",
       "      <td>-0.001603</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fish</th>\n",
       "      <td>-0.001603</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kebab</th>\n",
       "      <td>-0.001603</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>costcutter</th>\n",
       "      <td>-0.001603</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>waitrose</th>\n",
       "      <td>-0.001603</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sainsburys</th>\n",
       "      <td>-0.001603</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tesco</th>\n",
       "      <td>-0.001603</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grill</th>\n",
       "      <td>-0.001603</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.578</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            morans_EI morans_p morans_value\n",
       "cafe        -0.001603  0.00000        0.753\n",
       "coffee      -0.001603  0.00000        0.681\n",
       "pizza       -0.001603  0.00000        0.547\n",
       "wine        -0.001603  0.00000        0.625\n",
       "sushi       -0.001603  0.00000        0.437\n",
       "thai        -0.001603  0.00000        0.537\n",
       "chicken     -0.001603  0.00000        0.655\n",
       "fried       -0.001603  0.00000        0.592\n",
       "fish        -0.001603  0.00000        0.638\n",
       "kebab       -0.001603  0.00000        0.535\n",
       "costcutter  -0.001603  0.00000        0.443\n",
       "waitrose    -0.001603  0.00000        0.538\n",
       "sainsburys  -0.001603  0.00000        0.329\n",
       "tesco       -0.001603  0.00000        0.626\n",
       "grill       -0.001603  0.00000        0.578"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Local Moran's I\n",
    "\n",
    "for x in tokens_lst:\n",
    "\n",
    "    y = np.array(ward_variables[x + '_kde'])\n",
    "    lm = pysal.Moran_Local(y,w)\n",
    "    \n",
    "    ward_variables[x + '_lmoran_value'] = lm.Is\n",
    "    ward_variables[x + '_lmoran_p'] = lm.p_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Add colours for web mapping of morans I value \n",
    "\n",
    "def morans_color (row, df, p_var, lq_var, mor_var):\n",
    "    \n",
    "    subset_high_df = df[(df[p_var] <= 0.05) & (df[lq_var] > 1)]\n",
    "    subset_low_df = df[(df[p_var] <= 0.05) & (df[lq_var] < 1)]\n",
    "    \n",
    "    min_high = min(subset_high_df[mor_var])\n",
    "    max_high = max(subset_high_df[mor_var])\n",
    "    med_high = np.median(subset_high_df[mor_var])\n",
    "    \n",
    "    min_low = min(subset_low_df[mor_var])\n",
    "    max_low = max(subset_low_df[mor_var])\n",
    "    med_low = np.median(subset_low_df[mor_var])\n",
    "    \n",
    "    if row[p_var] > 0.05: #if not significant, colour grey\n",
    "        return '#acacac'\n",
    "    elif row[lq_var] < 1 and row[mor_var] < med_low: #if significant and less than median value for low concerntration color light blue\n",
    "        return '#00FFFF'\n",
    "    elif row[lq_var] < 1 and row[mor_var] >= med_low: #if significant and greater than median value for low color dark blue\n",
    "        return '#0066ff'\n",
    "    elif row[lq_var] > 1 and row[mor_var] < med_high: #if significant and less than median for high colow light pink\n",
    "        return '#ff00ff'\n",
    "    elif row[lq_var] > 1 and row[mor_var] >= med_high:\n",
    "        return '#cc0099'\n",
    "\n",
    "for x in tokens_lst:\n",
    "    ward_variables[x +'_morans_color'] = ward_variables.apply(lambda row: morans_color(row, \n",
    "                                                    ward_variables, x + '_lmoran_p', x + '_smooth_lq', x + '_lmoran_value'), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create dataframe for local morans values\n",
    "labels = ['very low value cluster lower limit', 'very low value cluster upper limit', 'low value cluster limit', \n",
    "          'not significant', 'high value cluster lower limit', 'high value cluster upper limit', 'very high value cluster limit']\n",
    "\n",
    "lmoran_legend_df = pd.DataFrame(index = labels)\n",
    "\n",
    "for x in tokens_lst:\n",
    "\n",
    "    subset_high_df = ward_variables[(ward_variables[x + '_lmoran_p'] <= 0.05) & (ward_variables[x + '_smooth_lq'] > 1)]\n",
    "    subset_low_df = ward_variables[(ward_variables[x + '_lmoran_p'] <= 0.05) & (ward_variables[x + '_smooth_lq'] < 1)]\n",
    "\n",
    "    min_high = min(subset_high_df[ x + '_lmoran_value'])\n",
    "    max_high = max(subset_high_df[ x + '_lmoran_value'])\n",
    "    med_high = np.median(subset_high_df[ x + '_lmoran_value'])\n",
    "\n",
    "    min_low = min(subset_low_df[ x + '_lmoran_value'])\n",
    "    max_low = max(subset_low_df[ x + '_lmoran_value'])\n",
    "    med_low = np.median(subset_low_df[ x + '_lmoran_value'])\n",
    "\n",
    "    values = [min_low, med_low, max_low, '0', min_high, med_high, max_high]\n",
    "\n",
    "    lmoran_legend_df[x + '_lmoran_value'] = values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lmoran_legend_df.to_csv('Data/local_morans_legend_values.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Visualise Local Morans Values\n",
    "\n",
    "for x in tokens_lst:\n",
    "    fig = plt.figure(figsize=(12,8))\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.setp(ax.get_xticklabels(), fontsize=8)\n",
    "    plt.setp(ax.get_yticklabels(), fontsize=8)\n",
    "    \n",
    "    cmap = plt.get_cmap('Purples')\n",
    "    test = PatchCollection(ward_patches, alpha=1, facecolor=cmap(ward_variables[x + '_lmoran_value']), lw = 0.1, \n",
    "                                          edgecolor = '0')\n",
    "    ax.add_collection(test)\n",
    "\n",
    "    ax.plot(df[x]['eastings'], df[x]['northings'], 'k.', markersize=4, markerfacecolor='green', alpha=0.8)\n",
    "    ax.set_xlim([xmin, xmax])\n",
    "    ax.set_ylim([ymin, ymax])\n",
    "    plt.title(\"Local Morans I Values for Word Token '\" + x +\"'\", weight = 'heavy', y=1.05)\n",
    "    ax.arrow(560000, 158000, 0, 4000, head_width=700, head_length=700, fc='k', ec='k')\n",
    "\n",
    "    m = cm.ScalarMappable(cmap=cm.Purples)\n",
    "    m.set_array(ward_variables[x + '_lmoran_value'])\n",
    "    plt.colorbar(m)\n",
    "\n",
    "    fig.savefig('spatial_analysis/figures/local_morans/' + x + '_local_morans.png', dpi=200, figsize = (12,8), transparent = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Visualise Local Morans P Values\n",
    "\n",
    "for x in tokens_lst:\n",
    "    \n",
    "    fig = plt.figure(figsize=(12,8))\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.setp(ax.get_xticklabels(), fontsize=8)\n",
    "    plt.setp(ax.get_yticklabels(), fontsize=8)\n",
    "    \n",
    "    cmap = plt.get_cmap('bwr')\n",
    "    test = PatchCollection(ward_patches, alpha=1, facecolor=cmap(ward_variables[x + '_lmoran_p']), lw = 0.1, \n",
    "                                          edgecolor = '0')\n",
    "    ax.add_collection(test)\n",
    "\n",
    "    ax.plot(df[x]['eastings'], df[x]['northings'], 'k.', markersize=4, markerfacecolor='green', alpha=0.8)\n",
    "    ax.set_xlim([xmin, xmax])\n",
    "    ax.set_ylim([ymin, ymax])\n",
    "    plt.title(\"Local Morans I P Values for Word Token '\" + x +\"'\", weight = 'heavy', y=1.05)\n",
    "    ax.arrow(560000, 158000, 0, 4000, head_width=700, head_length=700, fc='k', ec='k')\n",
    "\n",
    "    m = cm.ScalarMappable(cmap=cm.bwr)\n",
    "    m.set_array(ward_variables[x + '_lmoran_p'])\n",
    "    plt.colorbar(m)\n",
    "\n",
    "    fig.savefig('spatial_analysis/figures/local_morans/' + x + '_local_morans_p.png', dpi=200, figsize = (12,8), transparent=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7 - Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create scatter Matrix of Correlations\n",
    "\n",
    "#Subset Data\n",
    "lst = ['med_income_2012_13', 'combined_professionals_pct']\n",
    "\n",
    "subset_columns = []\n",
    "\n",
    "for x in lst:\n",
    "    subset_columns.append(x)\n",
    "\n",
    "for x in tokens_lst:\n",
    "    a = x + '_smooth_lq'\n",
    "    subset_columns.append(a)\n",
    "    \n",
    "ward_variables_subset = ward_variables[subset_columns]\n",
    "\n",
    "#Rename columns for visualisation\n",
    "for x in tokens_lst:\n",
    "    #Rename columns for visualisation\n",
    "    ward_variables_subset.rename(columns={x + '_smooth_lq': x}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# takes the dataset, an alpha value for opacity, a figure size setting, and a specification of the diagonal charts\n",
    "\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "fig = plt.figure(figsize=(13,13))\n",
    "a = pd.scatter_matrix(ward_variables_subset, alpha=0.2, diagonal='kde', figsize=(12,12))\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Calculate correlation coefficients\n",
    "a = ward_variables_subset.corr()\n",
    "subset = a.iloc[2:16]\n",
    "\n",
    "global_stats_df['income_correlation'] = subset['med_income_2012_13']\n",
    "global_stats_df['professional_pct_correlation'] = subset['combined_professionals_pct']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "global_stats_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8 - Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Regression - Income only\n",
    "\n",
    "w = pysal.rook_from_shapefile(\"Data/ESRI/london_wards.shp\")\n",
    "x = np.reshape(ward_variables['med_income_2012_13'], newshape=(625, 1))\n",
    "\n",
    "var = 'income'\n",
    "\n",
    "r2 = []\n",
    "ar2 = []\n",
    "moran_res = []\n",
    "coefficient = []\n",
    "t_stat = []\n",
    "\n",
    "for i in tokens_lst:\n",
    "    y = np.reshape(a=ward_variables[i + '_smooth_lq'],newshape=(625,1))\n",
    "    a = pysal.spreg.ols.OLS(y,x,w, spat_diag=True, moran=True)\n",
    "    ward_variables[i + '_' + var + '_residuals'] = a.u\n",
    "    \n",
    "    r2.append(a.r2)\n",
    "    ar2.append(a.ar2)\n",
    "    moran_res.append(a.moran_res)\n",
    "    coefficient.append(a.betas)\n",
    "    t_stat.append(a.t_stat)\n",
    "\n",
    "global_stats_df[var + '_r2'] = r2\n",
    "global_stats_df[var + '_ar2'] = ar2\n",
    "global_stats_df[var + '_moran_res'] = moran_res\n",
    "global_stats_df[var + '_coefficient'] = coefficient\n",
    "global_stats_df[var + '_t_stat'] = t_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Visualise income regression residuals\n",
    "\n",
    "for x in tokens_lst:\n",
    "    \n",
    "    fig = plt.figure(figsize=(12,8))\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.setp(ax.get_xticklabels(), fontsize=8)\n",
    "    plt.setp(ax.get_yticklabels(), fontsize=8)\n",
    "    \n",
    "    cmap = plt.get_cmap('bwr')\n",
    "    test = PatchCollection(ward_patches, alpha=1, facecolor=cmap(ward_variables[x + '_income_residuals']), lw = 0.1, \n",
    "                                          edgecolor = '0')\n",
    "    ax.add_collection(test)\n",
    "\n",
    "    ax.plot(df[x]['eastings'], df[x]['northings'], 'k.', markersize=4, markerfacecolor='green', alpha=0.8)\n",
    "    ax.set_xlim([xmin, xmax])\n",
    "    ax.set_ylim([ymin, ymax])\n",
    "    plt.title(\"Local Morans I P Values for Word Token '\" + x +\"'\", weight = 'heavy', y=1.05)\n",
    "    ax.arrow(560000, 158000, 0, 4000, head_width=700, head_length=700, fc='k', ec='k')\n",
    "\n",
    "    m = cm.ScalarMappable(cmap=cm.bwr)\n",
    "    m.set_array(ward_variables[x + '_income_residuals'])\n",
    "    plt.colorbar(m)\n",
    "\n",
    "    fig.savefig('spatial_analysis/figures/regression_residuals/' + x + '_income_residuals.png', dpi=200, figsize = (12,8), transparent=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Regression - proportion of population professional only\n",
    "\n",
    "w = pysal.rook_from_shapefile(\"Data/ESRI/london_wards.shp\")\n",
    "x = np.reshape(ward_variables['combined_professionals_pct'], newshape=(625, 1))\n",
    "\n",
    "var = 'professionals_pct'\n",
    "\n",
    "r2 = []\n",
    "ar2 = []\n",
    "moran_res = []\n",
    "coefficient = []\n",
    "t_stat = []\n",
    "\n",
    "for i in tokens_lst:\n",
    "    y = np.reshape(a=ward_variables[i + '_smooth_lq'],newshape=(625,1))\n",
    "    a = pysal.spreg.ols.OLS(y,x,w, spat_diag=True, moran=True)\n",
    "    ward_variables[i + '_' + var + '_residuals'] = a.u\n",
    "    \n",
    "    r2.append(a.r2)\n",
    "    ar2.append(a.ar2)\n",
    "    moran_res.append(a.moran_res)\n",
    "    coefficient.append(a.betas)\n",
    "    t_stat.append(a.t_stat)\n",
    "\n",
    "global_stats_df[var + '_r2'] = r2\n",
    "global_stats_df[var + '_ar2'] = ar2\n",
    "global_stats_df[var + '_moran_res'] = moran_res\n",
    "global_stats_df[var + '_coefficient'] = coefficient\n",
    "global_stats_df[var + '_t_stat'] = t_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Regression - proportion of population professional and income\n",
    "\n",
    "w = pysal.rook_from_shapefile(\"Data/ESRI/london_wards.shp\")\n",
    "x = ward_variables[['med_income_2012_13', 'combined_professionals_pct']].as_matrix()\n",
    "\n",
    "var = 'income_professionals'\n",
    "\n",
    "r2 = []\n",
    "ar2 = []\n",
    "moran_res = []\n",
    "coefficient = []\n",
    "t_stat = []\n",
    "\n",
    "for i in tokens_lst:\n",
    "    y = np.reshape(a=ward_variables[i + '_smooth_lq'],newshape=(625,1))\n",
    "    a = pysal.spreg.ols.OLS(y,x,w, spat_diag=True, moran=True)\n",
    "    ward_variables[i + '_' + var + '_residuals'] = a.u\n",
    "    \n",
    "    r2.append(a.r2)\n",
    "    ar2.append(a.ar2)\n",
    "    moran_res.append(a.moran_res)\n",
    "    coefficient.append(a.betas)\n",
    "    t_stat.append(a.t_stat)\n",
    "\n",
    "global_stats_df[var + '_r2'] = r2\n",
    "global_stats_df[var + '_ar2'] = ar2\n",
    "global_stats_df[var + '_moran_res'] = moran_res\n",
    "global_stats_df[var + '_coefficient'] = coefficient\n",
    "global_stats_df[var + '_t_stat'] = t_stat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9 - Explore Clustering Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list(ward_variables.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cluster_var = [['med_income_2012_13','coffee'],['med_income_2012_13', 'chicken'], ['coffee','chicken'],\n",
    "              ['med_income_2012_13','coffee','chicken'], ['med_income_2012_13','coffee','chicken', 'thai', 'kebab'], \n",
    "              ['coffee','chicken', 'thai', 'kebab']]\n",
    "\n",
    "cluster_names=['income_coffee', 'income_chicken', 'coffee_chicken', 'income_coffee_chicken', 'income_coffee_chicken_thai_kebab',\n",
    "              'coffee_chicken_thai_kebab']\n",
    "\n",
    "for i in range(len(cluster_var)):\n",
    "\n",
    "    for x in range(7):\n",
    "\n",
    "        from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "        subset = ward_variables_subset[cluster_var[i]]\n",
    "\n",
    "        import sklearn as sk\n",
    "        scaled = sk.preprocessing.scale(subset)\n",
    "        AgClustering = AgglomerativeClustering(n_clusters=x+1)\n",
    "        AgClustering.fit(scaled)\n",
    "        AgClustering_labels = AgClustering.labels_\n",
    "        ward_variables['ag_clusters'+ cluster_names[i] + str(x)] = AgClustering_labels\n",
    "\n",
    "        #cloropleth\n",
    "        plt.clf()\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111, axisbg='w', frame_on=False)\n",
    "        plt.setp(ax.get_xticklabels(), fontsize=8)\n",
    "        plt.setp(ax.get_yticklabels(), fontsize=8)\n",
    "\n",
    "        # use a blue colour ramp - we'll be converting it to a map using cmap()\n",
    "        cmap = plt.get_cmap('gist_ncar')\n",
    "        pc = PatchCollection(ward_patches, alpha=1, lw = 0.1, edgecolor = '0')\n",
    "\n",
    "        # impose our colour map onto the patch collection\n",
    "        norm = Normalize()\n",
    "        pc.set_facecolor(cmap(norm(ward_variables['ag_clusters' + cluster_names[i] + str(x)].values)))\n",
    "        ax.add_collection(pc)\n",
    "\n",
    "        ax.set_xlim([xmin, xmax])\n",
    "        ax.set_ylim([ymin, ymax])\n",
    "        plt.title('Heirachical Clustering for Variables: ' + cluster_names[i] + str(x), weight = 'heavy', y=1.05)\n",
    "        ax.arrow(560000, 158000, 0, 4000, head_width=700, head_length=700, fc='k', ec='k')\n",
    "\n",
    "        fig.savefig('spatial_analysis/figures/cluster/' + cluster_names[i] + str(x) + '_cluster.png', dpi=200, figsize = (12,8),\n",
    "                   transparent = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ward_variables['ag_clustersincome_coffee_chicken_thai_kebab3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ward_variables['ag_clustersincome_coffee_chicken_thai_kebab3']\n",
    "\n",
    "def cluster_colour (row, var):\n",
    "    if row[var] == 0:\n",
    "          return '#33ccff' #light blue\n",
    "    if row[var] == 1:\n",
    "          return '#ff5050' #orangy red\n",
    "    if row[var] == 2:\n",
    "          return '#66ff66' #green\n",
    "    if row[var] == 3:\n",
    "          return '#ffff66' #yellow\n",
    "\n",
    "#Apply function to create new dataframe coloumn\n",
    "ward_variables['cluster_colors'] = ward_variables.apply(lambda row: cluster_colour(row,'ag_clustersincome_coffee_chicken_thai_kebab3'),\n",
    "                                                        axis=1)\n",
    "print ward_variables['cluster_colors']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10 - Export .csvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ward_variables.to_csv('Data/spatial_analysis_wards_output.csv')\n",
    "global_stats_df.to_csv('Data/spatial_analysis_global_output.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Step 11 - Make charts for website content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Scatter Plots\n",
    "\n",
    "for i in range(len(tokens_lst)):\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    y = ward_variables['med_income_2012_13']\n",
    "    x = ward_variables[tokens_lst[i] + '_smooth_lq']\n",
    "\n",
    "    plt.setp(ax.get_xticklabels(), fontsize=8)\n",
    "    plt.setp(ax.get_yticklabels(), fontsize=8)\n",
    "\n",
    "    scatter = plt.scatter(x, y, marker='^', color = tokens_color[i]) \n",
    "    m, b = np.polyfit(x, y, deg=1)\n",
    "    plt.plot(x, m*x + b, '-', c='red')\n",
    "    ax.patch.set_alpha(0)\n",
    "\n",
    "    plt.title(\"Concerntration (LQ) of Token '\" + tokens_lst[i] + \"' by Income per Ward\", weight = 'normal', y=1.05)\n",
    "    plt.xlabel(\"Location Quotient\")\n",
    "    plt.ylabel('Income')\n",
    "\n",
    "\n",
    "    plt.xlim(xmin = 0)\n",
    "    plt.ylim(20000,90000)\n",
    "    \n",
    "    fig.savefig('spatial_analysis/figures/scatter_plot/' + tokens_lst[i] + '_income_scatter.png', dpi=200, figsize = (12,8),\n",
    "                   transparent = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Histograms\n",
    "\n",
    "\n",
    "for i in range (len(tokens_lst)):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    x = ward_variables[tokens_lst[i]  + '_smooth_lq']\n",
    "\n",
    "    plt.setp(ax.get_xticklabels(), fontsize=8)\n",
    "    plt.setp(ax.get_yticklabels(), fontsize=8)\n",
    "    n, bins, patches = plt.hist(x, 50, facecolor=tokens_color[i], alpha=0.75) \n",
    "    ax.patch.set_alpha(0)\n",
    "\n",
    "    plt.title(\"Histogram of Concentration (LQ) of Token '\" + tokens_lst[i]  + \"' per Ward\", weight = 'normal', y=1.05)\n",
    "    plt.xlabel(\"Location Quotient\")\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "\n",
    "    #plt.show()\n",
    "\n",
    "    fig.savefig('spatial_analysis/figures/histogram/' + tokens_lst[i] + '_histogram.png', dpi=200, figsize = (12,8),\n",
    "                       transparent = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ward_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
